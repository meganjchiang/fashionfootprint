{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Skort \n",
      "Invisible button fastening, one pocket at back, branded logo at back, high-rise fit \n",
      "Non-stretch material, unlined\n",
      "100% cotton\n",
      "Cold gentle machine wash\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def scrape_princess_polly(url):\n",
    "    try:\n",
    "        # Set Chrome options for headless mode\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "        # Initialize the WebDriver\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "        driver.get(url)\n",
    "        # Get the page source after interactions\n",
    "        page_source = driver.page_source\n",
    "        # Close the WebDriver once done to avoid multiple instances\n",
    "        driver.quit()\n",
    "        # Parse the page source with Beautiful Soup\n",
    "        soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "        # Extract data using Beautiful Soup methods\n",
    "        data_element = soup.find(\"div\", class_=\"product-details__content\")\n",
    "        if data_element:\n",
    "            return data_element.text\n",
    "        else:\n",
    "            return \"Data element not found on the page.\"\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "# URL of the webpage you want to scrape\n",
    "url = \"https://us.princesspolly.com/products/caruso-denim-wrap-skort-light-wash\"\n",
    "# Call the function to scrape the website\n",
    "data = scrape_princess_polly(url)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85% Polyamide, 15% Elastane\n"
     ]
    }
   ],
   "source": [
    "def scrape_shein(url):\n",
    "    try:\n",
    "        # Set Chrome options for headless mode\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "        # Initialize the WebDriver\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "        driver.get(url)\n",
    "        # Get the page source after interactions\n",
    "        page_source = driver.page_source\n",
    "        # Close the WebDriver once done to avoid multiple instances\n",
    "        driver.quit()\n",
    "        # Parse the page source with Beautiful Soup\n",
    "        soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "        # Find the div with class \"key\" containing \"Composition: \"\n",
    "        composition_key = soup.find(\"div\", class_=\"key\", string=\"Composition: \")\n",
    "        if composition_key:\n",
    "            # Get the next sibling of the \"key\" div, which contains the composition information\n",
    "            composition_val = composition_key.find_next_sibling(\"div\", class_=\"val\")\n",
    "            if composition_val:\n",
    "                # Extract the composition information\n",
    "                composition_text = composition_val.text.strip()\n",
    "                return composition_text\n",
    "            else:\n",
    "                return \"Composition information not found.\"\n",
    "        else:\n",
    "            return \"Composition key not found.\"\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "# URL of the webpage you want to scrape\n",
    "url = \"https://us.shein.com/Solid-Bandeau-Bra-p-12216010.html?src_module=All&src_identifier=on=PRODUCT_ITEMS_COMPONENT`cn=salezone`hz=0`ps=4_1_0`jc=itemPicking_100546960&src_tab_page_id=page_home1713242748965&mallCode=1&imgRatio=3-4\"\n",
    "# Call the function to scrape the website\n",
    "data = scrape_shein(url)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Made of soft fleece fabric this hoodie has a cozy full coverage hood, dropped shoulders for a relaxed fit, a roomy cut that makes layering easy, and the kangaroo pocket that helps keep your hands nice and toasty or can provide a spot to stash small items, making this top a comfy, modern wardrobe essential showcasing an iconic collaboration. Pair with any of the Jordan Paris Saint Germain bottoms for a coordinated look.Product Details80% Cotton, 20% PolyesterMachine WashImportedShown: BlackStyle: 45C566-023\n"
     ]
    }
   ],
   "source": [
    "def scrape_nike(url):\n",
    "    try:\n",
    "        # Set Chrome options for headless mode\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "        # Initialize the WebDriver\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "        driver.get(url)\n",
    "        # Get the page source after interactions\n",
    "        page_source = driver.page_source\n",
    "        # Close the WebDriver once done to avoid multiple instances\n",
    "        driver.quit()\n",
    "        # Parse the page source with Beautiful Soup\n",
    "        soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "        # Extract data using Beautiful Soup methods\n",
    "        data_element = soup.find(\"div\", class_=\"pi-pdpmainbody\")\n",
    "        if data_element:\n",
    "            return data_element.text\n",
    "        else:\n",
    "            return \"Data element not found on the page.\"\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "    \n",
    "# URL of the webpage you want to scrape\n",
    "url = \"https://www.nike.com/t/jordan-paris-saint-germain-pullover-hoodie-big-kids-hoodie-tpqdWh/45C566-023?nikemt=true&cp=75009274456_search_&gad_source=1&gclid=CjwKCAjwoPOwBhAeEiwAJuXRh47Ul92JN0-vlgv6RcIG0mKBklxgk4gs4zOPV8cguxRM7hK9eK_DThoCLUoQAvD_BwE\"\n",
    "# Call the function to scrape the website\n",
    "data = scrape_nike(url)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted, short sleeve ribbed cotton top with a scoop neck collar.Fabrics: 100% cotton Measurements: 18\" (46 cm) length, 16\" (41 cm) bust Made in: Italy\n"
     ]
    }
   ],
   "source": [
    "def scrape_brandy(url):\n",
    "    try:\n",
    "        # Set Chrome options for headless mode\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "        \n",
    "        # Initialize the WebDriver\n",
    "        with webdriver.Chrome(options=chrome_options) as driver:\n",
    "            driver.get(url)\n",
    "            \n",
    "            # Get the page source after interactions\n",
    "            page_source = driver.page_source\n",
    "        \n",
    "        # Parse the page source with Beautiful Soup\n",
    "        soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "        \n",
    "        # Find the <div> tag with class \"product__description rte\"\n",
    "        data_element = soup.find(\"div\", class_=\"product__description rte\")\n",
    "        \n",
    "        if data_element:\n",
    "            return data_element.text.strip()\n",
    "        else:\n",
    "            return \"Data element not found on the page.\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "# URL of the webpage you want to scrape\n",
    "url = \"https://us.brandymelville.com/products/zelly-basic-top\"\n",
    "# Call the function to scrape the website\n",
    "data = scrape_brandy(url)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Body:100% CottonMachine wash cold, with like colorsOnly non-chlorine bleachTumble dry lowWarm iron if neededDo not dry clean\n"
     ]
    }
   ],
   "source": [
    "def scrape_abercrombie(url):\n",
    "    try:\n",
    "        # Set Chrome options for headless mode\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "        \n",
    "        # Initialize the WebDriver\n",
    "        with webdriver.Chrome(options=chrome_options) as driver:\n",
    "            driver.get(url)\n",
    "            \n",
    "            # Get the page source after interactions\n",
    "            page_source = driver.page_source\n",
    "        \n",
    "        # Parse the page source with Beautiful Soup\n",
    "        soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "        \n",
    "        # Find the <div> tag with class \"product__description rte\"\n",
    "        data_element = soup.find(\"div\", class_=\"fabric-care-mfe\")\n",
    "        \n",
    "        if data_element:\n",
    "            return data_element.text.strip()\n",
    "        else:\n",
    "            return \"Data element not found on the page.\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "# URL of the webpage you want to scrape\n",
    "url = \"https://www.abercrombie.com/shop/us/p/oversized-gauzy-shirt-53457331?categoryId=12203&faceout=model&seq=01\"\n",
    "# Call the function to scrape the website\n",
    "data = scrape_abercrombie(url)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fabric type \n",
      "  \n",
      "75% Nylon, 25% Elastane\n"
     ]
    }
   ],
   "source": [
    "def scrape_amazon(url):\n",
    "    try:\n",
    "        # Set Chrome options for headless mode\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "        \n",
    "        # Initialize the WebDriver\n",
    "        with webdriver.Chrome(options=chrome_options) as driver:\n",
    "            driver.get(url)\n",
    "            \n",
    "            # Get the page source after interactions\n",
    "            page_source = driver.page_source\n",
    "        \n",
    "        # Parse the page source with Beautiful Soup\n",
    "        soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "        \n",
    "        # Find the <div> tag with class \"product__description rte\"\n",
    "        data_element = soup.find(\"div\", class_=\"a-fixed-left-grid product-facts-detail\")\n",
    "        \n",
    "        if data_element:\n",
    "            return data_element.text.strip()\n",
    "        else:\n",
    "            return \"Data element not found on the page.\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "# URL of the webpage you want to scrape\n",
    "url = \"https://www.amazon.com/Lyss%C3%A9-Womens-Schiffer-Button-Medium/dp/B01EWRKXTE/ref=lp_121173939011_1_2?pf_rd_p=53d84f87-8073-4df1-9740-1bf3fa798149&pf_rd_r=VN92AA0AZ4XSEQDKW4EJ\"\n",
    "# Call the function to scrape the website\n",
    "data = scrape_amazon(url)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stretch, plain-woven fabric\n",
      "\n",
      "Main: 76% Polyester, 18% Viscose, 6% Elastane.\n"
     ]
    }
   ],
   "source": [
    "def scrape_asos(url, interactive_element_xpath, loaded_content_xpath, wait_time=10):\n",
    "    try:\n",
    "        # Set Chrome options for headless mode\n",
    "        chrome_options = Options()\n",
    "\n",
    "        # Initialize the WebDriver with headless mode\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "        \n",
    "        # Open the webpage\n",
    "        driver.get(url)\n",
    "\n",
    "        # Wait for the specified time before clicking the interactive element\n",
    "        time.sleep(wait_time)  # Wait for the specified time in seconds\n",
    "\n",
    "        # Find the interactive element\n",
    "        interactive_element = driver.find_element(By.XPATH, interactive_element_xpath)\n",
    "        \n",
    "        # Click the interactive element\n",
    "        interactive_element.click()\n",
    "\n",
    "        # Wait for the loaded content to be visible\n",
    "        loaded_element = WebDriverWait(driver, wait_time).until(\n",
    "            EC.visibility_of_element_located((By.XPATH, loaded_content_xpath))\n",
    "        )\n",
    "\n",
    "        # Once loaded, scrape the content\n",
    "        dynamic_content = loaded_element.text\n",
    "        \n",
    "        return dynamic_content\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None\n",
    "        \n",
    "    finally:\n",
    "        # Close the WebDriver\n",
    "        driver.quit()\n",
    "\n",
    "# Example usage:\n",
    "url = \"https://www.asos.com/us/asos-design/asos-design-pleated-mini-skirt-in-gray/prd/205661997#colourWayId-205661998\"\n",
    "interactive_element_xpath = '//*[@id=\"productDescription\"]/ul/li[5]/div/h2'\n",
    "loaded_content_xpath = '//*[@id=\"productDescriptionAboutMe\"]'\n",
    "\n",
    "dynamic_content = scrape_asos(url, interactive_element_xpath, loaded_content_xpath)\n",
    "if dynamic_content:\n",
    "    print(dynamic_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Shell: 96% ramie, 4% other fibers\n",
      "- Lining: 100% polyester\n",
      "- Hand wash cold\n"
     ]
    }
   ],
   "source": [
    "def scrape_f21(url, target_element_xpath, wait_time=10):\n",
    "    try:\n",
    "        # Set Chrome options for headless mode\n",
    "        chrome_options = Options()\n",
    "\n",
    "        # Initialize the WebDriver with headless mode\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "        \n",
    "        # Open the webpage\n",
    "        driver.get(url)\n",
    "\n",
    "        # Wait for the target element to be visible\n",
    "        target_element = WebDriverWait(driver, wait_time).until(\n",
    "            EC.visibility_of_element_located((By.XPATH, target_element_xpath))\n",
    "        )\n",
    "\n",
    "        # Once loaded, scrape the content\n",
    "        dynamic_content = target_element.text\n",
    "        \n",
    "        return dynamic_content\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None\n",
    "        \n",
    "    finally:\n",
    "        # Close the WebDriver\n",
    "        driver.quit()\n",
    "\n",
    "# Example usage:\n",
    "url = \"https://www.forever21.com/us/20012622040501.html\"\n",
    "target_element_xpath = '//*[@id=\"main\"]/div[2]/div[1]/div[2]/div[3]/div/div[6]/div/div[1]/section[2]/div'\n",
    "\n",
    "dynamic_content = scrape_f21(url, target_element_xpath)\n",
    "if dynamic_content:\n",
    "    print(dynamic_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Materials & Care\n",
      "94% Cotton, 5% Recycled Cotton, 1% Elastane\n",
      "Color may transfer when new. Wash once separately in cold water before wearing. Machine wash cold inside out with like colors. Do not bleach. Tumble dry low. Cool iron if needed.\n",
      "Imported\n"
     ]
    }
   ],
   "source": [
    "def scrape_asos(url, interactive_element_xpath, loaded_content_xpath, wait_time=10):\n",
    "    try:\n",
    "        # Set Chrome options for headless mode\n",
    "        chrome_options = Options()\n",
    "\n",
    "        # Initialize the WebDriver with headless mode\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "        \n",
    "        # Open the webpage\n",
    "        driver.get(url)\n",
    "\n",
    "        # Wait for the specified time before clicking the interactive element\n",
    "        time.sleep(wait_time)  # Wait for the specified time in seconds\n",
    "\n",
    "        # Find the interactive element\n",
    "        interactive_element = driver.find_element(By.XPATH, interactive_element_xpath)\n",
    "        \n",
    "        # Click the interactive element\n",
    "        interactive_element.click()\n",
    "\n",
    "        # Wait for the loaded content to be visible\n",
    "        loaded_element = WebDriverWait(driver, wait_time).until(\n",
    "            EC.visibility_of_element_located((By.XPATH, loaded_content_xpath))\n",
    "        )\n",
    "\n",
    "        # Once loaded, scrape the content\n",
    "        dynamic_content = loaded_element.text\n",
    "        \n",
    "        return dynamic_content\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None\n",
    "        \n",
    "    finally:\n",
    "        # Close the WebDriver\n",
    "        driver.quit()\n",
    "\n",
    "# Example usage:\n",
    "url = \"https://www.ae.com/us/en/p/women/high-waisted-jeans/high-waisted-mom-jeans/ae-stretch-mom-jean/0436_4332_110?menu=cat4840004\"\n",
    "interactive_element_xpath = '//*[@id=\"main-content-focus\"]/div[2]/div[2]/div[2]/div/div[3]/div/div[1]/div[1]'\n",
    "loaded_content_xpath = '//*[@id=\"main-content-focus\"]/div[2]/div[2]/div[2]/div/div[3]/div/div[1]/div[2]/div/div[2]'\n",
    "\n",
    "dynamic_content = scrape_asos(url, interactive_element_xpath, loaded_content_xpath)\n",
    "if dynamic_content:\n",
    "    print(dynamic_content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
