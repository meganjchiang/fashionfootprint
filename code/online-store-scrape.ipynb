{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from lxml import html\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Skort \n",
      "Invisible button fastening, one pocket at back, branded logo at back, high-rise fit \n",
      "Non-stretch material, unlined\n",
      "100% cotton\n",
      "Cold gentle machine wash\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def scrape_princess_polly(url):\n",
    "    try:\n",
    "        # Set Chrome options for headless mode\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "        # Initialize the WebDriver\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "        driver.get(url)\n",
    "        # Get the page source after interactions\n",
    "        page_source = driver.page_source\n",
    "        # Close the WebDriver once done to avoid multiple instances\n",
    "        driver.quit()\n",
    "        # Parse the page source with Beautiful Soup\n",
    "        soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "        # Extract data using Beautiful Soup methods\n",
    "        data_element = soup.find(\"div\", class_=\"product-details__content\")\n",
    "        if data_element:\n",
    "            return data_element.text\n",
    "        else:\n",
    "            return \"Data element not found on the page.\"\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "# URL of the webpage you want to scrape\n",
    "url = \"https://us.princesspolly.com/products/caruso-denim-wrap-skort-light-wash\"\n",
    "# Call the function to scrape the website\n",
    "data = scrape_princess_polly(url)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85% Polyamide, 15% Elastane\n"
     ]
    }
   ],
   "source": [
    "def scrape_shein(url):\n",
    "    try:\n",
    "        # Set Chrome options for headless mode\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "        # Initialize the WebDriver\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "        driver.get(url)\n",
    "        # Get the page source after interactions\n",
    "        page_source = driver.page_source\n",
    "        # Close the WebDriver once done to avoid multiple instances\n",
    "        driver.quit()\n",
    "        # Parse the page source with Beautiful Soup\n",
    "        soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "        # Find the div with class \"key\" containing \"Composition: \"\n",
    "        composition_key = soup.find(\"div\", class_=\"key\", string=\"Composition: \")\n",
    "        if composition_key:\n",
    "            # Get the next sibling of the \"key\" div, which contains the composition information\n",
    "            composition_val = composition_key.find_next_sibling(\"div\", class_=\"val\")\n",
    "            if composition_val:\n",
    "                # Extract the composition information\n",
    "                composition_text = composition_val.text.strip()\n",
    "                return composition_text\n",
    "            else:\n",
    "                return \"Composition information not found.\"\n",
    "        else:\n",
    "            return \"Composition key not found.\"\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "# URL of the webpage you want to scrape\n",
    "url = \"https://us.shein.com/Solid-Bandeau-Bra-p-12216010.html?src_module=All&src_identifier=on=PRODUCT_ITEMS_COMPONENT`cn=salezone`hz=0`ps=4_1_0`jc=itemPicking_100546960&src_tab_page_id=page_home1713242748965&mallCode=1&imgRatio=3-4\"\n",
    "# Call the function to scrape the website\n",
    "data = scrape_shein(url)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Made of soft fleece fabric this hoodie has a cozy full coverage hood, dropped shoulders for a relaxed fit, a roomy cut that makes layering easy, and the kangaroo pocket that helps keep your hands nice and toasty or can provide a spot to stash small items, making this top a comfy, modern wardrobe essential showcasing an iconic collaboration. Pair with any of the Jordan Paris Saint Germain bottoms for a coordinated look.Product Details80% Cotton, 20% PolyesterMachine WashImportedShown: BlackStyle: 45C566-023\n"
     ]
    }
   ],
   "source": [
    "def scrape_nike(url):\n",
    "    try:\n",
    "        # Set Chrome options for headless mode\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "        # Initialize the WebDriver\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "        driver.get(url)\n",
    "        # Get the page source after interactions\n",
    "        page_source = driver.page_source\n",
    "        # Close the WebDriver once done to avoid multiple instances\n",
    "        driver.quit()\n",
    "        # Parse the page source with Beautiful Soup\n",
    "        soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "        # Extract data using Beautiful Soup methods\n",
    "        data_element = soup.find(\"div\", class_=\"pi-pdpmainbody\")\n",
    "        if data_element:\n",
    "            return data_element.text\n",
    "        else:\n",
    "            return \"Data element not found on the page.\"\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "    \n",
    "# URL of the webpage you want to scrape\n",
    "url = \"https://www.nike.com/t/jordan-paris-saint-germain-pullover-hoodie-big-kids-hoodie-tpqdWh/45C566-023?nikemt=true&cp=75009274456_search_&gad_source=1&gclid=CjwKCAjwoPOwBhAeEiwAJuXRh47Ul92JN0-vlgv6RcIG0mKBklxgk4gs4zOPV8cguxRM7hK9eK_DThoCLUoQAvD_BwE\"\n",
    "# Call the function to scrape the website\n",
    "data = scrape_nike(url)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted, short sleeve ribbed cotton top with a scoop neck collar.Fabrics: 100% cotton Measurements: 18\" (46 cm) length, 16\" (41 cm) bust Made in: Italy\n"
     ]
    }
   ],
   "source": [
    "def scrape_brandy(url):\n",
    "    try:\n",
    "        # Set Chrome options for headless mode\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "        \n",
    "        # Initialize the WebDriver\n",
    "        with webdriver.Chrome(options=chrome_options) as driver:\n",
    "            driver.get(url)\n",
    "            \n",
    "            # Get the page source after interactions\n",
    "            page_source = driver.page_source\n",
    "        \n",
    "        # Parse the page source with Beautiful Soup\n",
    "        soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "        \n",
    "        # Find the <div> tag with class \"product__description rte\"\n",
    "        data_element = soup.find(\"div\", class_=\"product__description rte\")\n",
    "        \n",
    "        if data_element:\n",
    "            return data_element.text.strip()\n",
    "        else:\n",
    "            return \"Data element not found on the page.\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "# URL of the webpage you want to scrape\n",
    "url = \"https://us.brandymelville.com/products/zelly-basic-top\"\n",
    "# Call the function to scrape the website\n",
    "data = scrape_brandy(url)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pocket Bag:70% Polyester, 30% Cotton / Body:78% Cotton, 22% LyocellTo avoid color transfer, wash before wear, turn garment inside outMachine wash cold, with like colorsDo not bleachTumble dry lowWarm iron if neededDo not dry clean\n"
     ]
    }
   ],
   "source": [
    "def scrape_abercrombie(url):\n",
    "    try:\n",
    "        # Set Chrome options for headless mode\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "        \n",
    "        # Initialize the WebDriver\n",
    "        with webdriver.Chrome(options=chrome_options) as driver:\n",
    "            driver.get(url)\n",
    "            \n",
    "            # Wait for the page to fully load\n",
    "            WebDriverWait(driver, 25).until(EC.presence_of_element_located((By.CSS_SELECTOR, 'body')))\n",
    "            \n",
    "            # Get the page source after interactions\n",
    "            page_source = driver.page_source\n",
    "        \n",
    "        # Parse the page source with Beautiful Soup\n",
    "        soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "        \n",
    "        # Find the <div> tag with class \"product__description rte\"\n",
    "        data_element = soup.find(\"h4\", class_=\"h4 fabric-care-mfe__label\")\n",
    "        \n",
    "        if data_element:\n",
    "            return data_element.text.strip()\n",
    "        else:\n",
    "            return \"Link no longer available.\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "# URL of the webpage you want to scrape\n",
    "url = \"https://bit.ly/3UlUXwr\"\n",
    "# Call the function to scrape the website\n",
    "data = scrape_abercrombie(url)\n",
    "print(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fabric type \n",
      "  \n",
      "75% Nylon, 25% Elastane\n"
     ]
    }
   ],
   "source": [
    "def scrape_amazon(url):\n",
    "    try:\n",
    "        # Set Chrome options for headless mode\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "        \n",
    "        # Initialize the WebDriver\n",
    "        with webdriver.Chrome(options=chrome_options) as driver:\n",
    "            driver.get(url)\n",
    "            \n",
    "            # Get the page source after interactions\n",
    "            page_source = driver.page_source\n",
    "        \n",
    "        # Parse the page source with Beautiful Soup\n",
    "        soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "        \n",
    "        # Find the <div> tag with class \"product__description rte\"\n",
    "        data_element = soup.find(\"div\", class_=\"a-fixed-left-grid product-facts-detail\")\n",
    "        \n",
    "        if data_element:\n",
    "            return data_element.text.strip()\n",
    "        else:\n",
    "            return \"Data element not found on the page.\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "# URL of the webpage you want to scrape\n",
    "url = \"https://www.amazon.com/Lyss%C3%A9-Womens-Schiffer-Button-Medium/dp/B01EWRKXTE/ref=lp_121173939011_1_2?pf_rd_p=53d84f87-8073-4df1-9740-1bf3fa798149&pf_rd_r=VN92AA0AZ4XSEQDKW4EJ\"\n",
    "# Call the function to scrape the website\n",
    "data = scrape_amazon(url)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stretch, plain-woven fabric\n",
      "\n",
      "Main: 76% Polyester, 18% Viscose, 6% Elastane.\n"
     ]
    }
   ],
   "source": [
    "def scrape_asos(url, interactive_element_xpath, loaded_content_xpath, wait_time=10):\n",
    "    try:\n",
    "        # Set Chrome options for headless mode\n",
    "        chrome_options = Options()\n",
    "\n",
    "        # Initialize the WebDriver with headless mode\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "        \n",
    "        # Open the webpage\n",
    "        driver.get(url)\n",
    "\n",
    "        # Wait for the specified time before clicking the interactive element\n",
    "        time.sleep(wait_time)  # Wait for the specified time in seconds\n",
    "\n",
    "        # Find the interactive element\n",
    "        interactive_element = driver.find_element(By.XPATH, interactive_element_xpath)\n",
    "        \n",
    "        # Click the interactive element\n",
    "        interactive_element.click()\n",
    "\n",
    "        # Wait for the loaded content to be visible\n",
    "        loaded_element = WebDriverWait(driver, wait_time).until(\n",
    "            EC.visibility_of_element_located((By.XPATH, loaded_content_xpath))\n",
    "        )\n",
    "\n",
    "        # Once loaded, scrape the content\n",
    "        dynamic_content = loaded_element.text\n",
    "        \n",
    "        return dynamic_content\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None\n",
    "        \n",
    "    finally:\n",
    "        # Close the WebDriver\n",
    "        driver.quit()\n",
    "\n",
    "# Example usage:\n",
    "url = \"https://www.asos.com/us/asos-design/asos-design-pleated-mini-skirt-in-gray/prd/205661997#colourWayId-205661998\"\n",
    "interactive_element_xpath = '//*[@id=\"productDescription\"]/ul/li[5]/div/h2'\n",
    "loaded_content_xpath = '//*[@id=\"productDescriptionAboutMe\"]'\n",
    "\n",
    "dynamic_content = scrape_asos(url, interactive_element_xpath, loaded_content_xpath)\n",
    "if dynamic_content:\n",
    "    print(dynamic_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Shell: 96% ramie, 4% other fibers\n",
      "- Lining: 100% polyester\n",
      "- Hand wash cold\n"
     ]
    }
   ],
   "source": [
    "def scrape_f21(url, target_element_xpath, wait_time=10):\n",
    "    try:\n",
    "        # Set Chrome options for headless mode\n",
    "        chrome_options = Options()\n",
    "\n",
    "        # Initialize the WebDriver with headless mode\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "        \n",
    "        # Open the webpage\n",
    "        driver.get(url)\n",
    "\n",
    "        # Wait for the target element to be visible\n",
    "        target_element = WebDriverWait(driver, wait_time).until(\n",
    "            EC.visibility_of_element_located((By.XPATH, target_element_xpath))\n",
    "        )\n",
    "\n",
    "        # Once loaded, scrape the content\n",
    "        dynamic_content = target_element.text\n",
    "        \n",
    "        return dynamic_content\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None\n",
    "        \n",
    "    finally:\n",
    "        # Close the WebDriver\n",
    "        driver.quit()\n",
    "\n",
    "# Example usage:\n",
    "url = \"https://www.forever21.com/us/20012622040501.html\"\n",
    "target_element_xpath = '//*[@id=\"main\"]/div[2]/div[1]/div[2]/div[3]/div/div[6]/div/div[1]/section[2]/div'\n",
    "\n",
    "dynamic_content = scrape_f21(url, target_element_xpath)\n",
    "if dynamic_content:\n",
    "    print(dynamic_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Materials & Care\n",
      "94% Cotton, 5% Recycled Cotton, 1% Elastane\n",
      "Color may transfer when new. Wash once separately in cold water before wearing. Machine wash cold inside out with like colors. Do not bleach. Tumble dry low. Cool iron if needed.\n",
      "Imported\n"
     ]
    }
   ],
   "source": [
    "def scrape_american_eagle(url, interactive_element_xpath, loaded_content_xpath, wait_time=10):\n",
    "    try:\n",
    "        # Set Chrome options for headless mode\n",
    "        chrome_options = Options()\n",
    "\n",
    "        # Initialize the WebDriver with headless mode\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "        \n",
    "        # Open the webpage\n",
    "        driver.get(url)\n",
    "\n",
    "        # Wait for the specified time before clicking the interactive element\n",
    "        time.sleep(wait_time)  # Wait for the specified time in seconds\n",
    "\n",
    "        # Find the interactive element\n",
    "        interactive_element = driver.find_element(By.XPATH, interactive_element_xpath)\n",
    "        \n",
    "        # Click the interactive element\n",
    "        interactive_element.click()\n",
    "\n",
    "        # Wait for the loaded content to be visible\n",
    "        loaded_element = WebDriverWait(driver, wait_time).until(\n",
    "            EC.visibility_of_element_located((By.XPATH, loaded_content_xpath))\n",
    "        )\n",
    "\n",
    "        # Once loaded, scrape the content\n",
    "        dynamic_content = loaded_element.text\n",
    "        \n",
    "        return dynamic_content\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None\n",
    "        \n",
    "    finally:\n",
    "        # Close the WebDriver\n",
    "        driver.quit()\n",
    "\n",
    "# Example usage:\n",
    "url = \"https://www.ae.com/us/en/p/women/high-waisted-jeans/high-waisted-mom-jeans/ae-stretch-mom-jean/0436_4332_110?menu=cat4840004\"\n",
    "interactive_element_xpath = '//*[@id=\"main-content-focus\"]/div[2]/div[2]/div[2]/div/div[3]/div/div[1]/div[1]'\n",
    "loaded_content_xpath = '//*[@id=\"main-content-focus\"]/div[2]/div[2]/div[2]/div/div[3]/div/div[1]/div[2]/div/div[2]'\n",
    "\n",
    "dynamic_content = scrape_american_eagle(url, interactive_element_xpath, loaded_content_xpath)\n",
    "if dynamic_content:\n",
    "    print(dynamic_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fabrication\n",
      "Medium-compression signature Airbrush performance fabric \n",
      "87% Nylon, 13% Elastane\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def scrape_alo(url):\n",
    "    try:\n",
    "        # Set Chrome options for headless mode\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "        # Initialize the WebDriver\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "        driver.get(url)\n",
    "        # Get the page source after interactions\n",
    "        page_source = driver.page_source\n",
    "        # Close the WebDriver once done to avoid multiple instances\n",
    "        driver.quit()\n",
    "        # Parse the page source with Beautiful Soup\n",
    "        soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "        # Extract data using Beautiful Soup methods\n",
    "        data_element = soup.find(\"div\", class_=\"fabrication\")\n",
    "        if data_element:\n",
    "            return data_element.text\n",
    "        else:\n",
    "            return \"Data element not found on the page.\"\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "# URL of the webpage you want to scrape\n",
    "url = \"https://www.aloyoga.com/products/w9538r-airbrush-stream-lined-bra-tank-ivory-black?variant=41346126512308&disableCurrencyEstimate&gad_source=1&gclid=CjwKCAjww_iwBhApEiwAuG6ccL77JcerRoX7_JAHcpKdcfgONVm-BMBBTVNxENk7-9cIIDnlReDxGxoCaegQAvD_BwE\"\n",
    "# Call the function to scrape the website\n",
    "data = scrape_alo(url)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a deadstock woven fabric made from 72% Polyester, 15% Rayon, 5% Wool, 5% Viscose, and 3% Spandex. Dry clean only. \n",
      "This is a deadstock woven fabric made from 72% Polyester, 15% Rayon, 5% Wool, 5% Viscose, and 3% Spandex.\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def scrape_reformation(url):\n",
    "    try:\n",
    "        page = requests.get(url)\n",
    "        soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "        composition_elements = soup.find_all(string=lambda text: '% ' in str(text).lower())\n",
    "        if composition_elements:\n",
    "            for element in composition_elements:\n",
    "                print(element)\n",
    "        else:\n",
    "            print(\"none\")\n",
    "    except Exception as e:\n",
    "        return f\"An error occured {str(e)}\"\n",
    "    \n",
    "url = \"https://www.thereformation.com/products/vida-low-rise-pant/1314838BLK.html?dwvar_1314838BLK_color=BLK&_gl=1*13y89uh*_up*MQ..&gclid=CjwKCAjww_iwBhApEiwAuG6ccHuReQ50wHf1NGMlZG5XTJoqFuugi5BXX5mlzOcjB35u29kXoUvEwBoCbf4QAvD_BwE&atc_confirmation=true\"\n",
    "data = scrape_reformation(url)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Fringe scarf is crafted from a rich wool mohair blend, detailed with an Acne Studios embroidered logo label and weaved contrast framing.\n",
      "Embroidered logo label\n",
      "Weaved contrast frame\n",
      "Measurements: 28 cm; 250 cm\n",
      "Fringe measurements: 12 cm approx.\n",
      "Made in Italy\n",
      "Style ID: FN-UX-SCAR000328\n",
      "\n",
      "\n",
      "Shell: 33% Alpaca, 25% Wool, 22% Nylon, 20% Mohair\n",
      "\n",
      "\n",
      "\n",
      "Show more\n",
      "Show less\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def scrape_acne_studios(url):\n",
    "    try:\n",
    "        # Set Chrome options for headless mode\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "        # Initialize the WebDriver\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "        driver.get(url)\n",
    "        # Get the page source after interactions\n",
    "        page_source = driver.page_source\n",
    "        # Close the WebDriver once done to avoid multiple instances\n",
    "        driver.quit()\n",
    "        # Parse the page source with Beautiful Soup\n",
    "        soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "        # Extract data using Beautiful Soup methods\n",
    "        data_element = soup.find(\"div\", id=\"productDescription\")\n",
    "        if data_element:\n",
    "            return data_element.text\n",
    "        else:\n",
    "            return \"Data element not found on the page.\"\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "# URL of the webpage you want to scrape\n",
    "url = \"https://www.acnestudios.com/us/en/wool-mohair-scarf---narrow-lavender-purple/CA0290-ADH.html?g=woman\"\n",
    "# Call the function to scrape the website\n",
    "data = scrape_acne_studios(url)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Details\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Maxi dress\n",
      " Bustier\n",
      " Tie straps\n",
      " Tiered skirt with asymmetric hem\n",
      " Exposed zipper back closure\n",
      " 97% cotton, 3% elastane\n",
      " Dry clean only \n",
      " Imported\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def scrape_alice_and_olivia(url):\n",
    "    try:\n",
    "        # Set Chrome options for headless mode\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "        # Initialize the WebDriver\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "        driver.get(url)\n",
    "        # Get the page source after interactions\n",
    "        page_source = driver.page_source\n",
    "        # Close the WebDriver once done to avoid multiple instances\n",
    "        driver.quit()\n",
    "        # Parse the page source with Beautiful Soup\n",
    "        soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "        # Extract data using Beautiful Soup methods\n",
    "        data_element = soup.find(\"div\", class_=\"details\")\n",
    "        if data_element:\n",
    "            return data_element.text\n",
    "        else:\n",
    "            return \"Data element not found on the page.\"\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "# URL of the webpage you want to scrape\n",
    "url = \"https://www.aliceandolivia.com/rosalee-tie-strap-bustier-maxi-dress/CC404P22523G123.html?lang=default\"\n",
    "# Call the function to scrape the website\n",
    "data = scrape_alice_and_olivia(url)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65% Polyester, 35% Cotton\n"
     ]
    }
   ],
   "source": [
    "def scrape_sandy_liang(url):\n",
    "    try:\n",
    "        # Set Chrome options for headless mode\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "        \n",
    "        # Initialize the WebDriver\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "        \n",
    "        # Open the webpage\n",
    "        driver.get(url)\n",
    "        \n",
    "        # Wait for the page to fully load\n",
    "        time.sleep(5)  # Adjust the wait time as needed\n",
    "        \n",
    "        # Get the page source after interactions\n",
    "        page_source = driver.page_source\n",
    "        \n",
    "        # Close the WebDriver once done to avoid multiple instances\n",
    "        driver.quit()\n",
    "        \n",
    "        # Parse the page source with lxml and XPath\n",
    "        tree = html.fromstring(page_source)\n",
    "        \n",
    "        # Extract data using XPath expressions\n",
    "        data_element = tree.xpath('//*[@id=\"product-collapse-description\"]/p[3]/span/span/text()')\n",
    "        \n",
    "        if data_element:\n",
    "            return ''.join(data_element).strip()\n",
    "        else:\n",
    "            return \"Data element not found on the page.\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "# URL of the webpage you want to scrape\n",
    "url = \"https://www.sandyliang.info/products/primo-dress\"\n",
    "# Call the function to scrape the website\n",
    "data = scrape_sandy_liang(url)\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49% Cotton, 5% Elastane, 46% Polyester\n"
     ]
    }
   ],
   "source": [
    "def scrape_billabong(url):\n",
    "    try:\n",
    "        # Set Chrome options for headless mode\n",
    "        chrome_options = ChromeOptions()\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "        \n",
    "        # Initialize the WebDriver\n",
    "        driver = Chrome(options=chrome_options)\n",
    "        \n",
    "        # Open the webpage\n",
    "        driver.get(url)\n",
    "        \n",
    "        # Wait for the page to fully load\n",
    "        time.sleep(5)  # Adjust the wait time as needed\n",
    "        \n",
    "        # Get the page source after interactions\n",
    "        page_source = driver.page_source\n",
    "        \n",
    "        # Close the WebDriver once done to avoid multiple instances\n",
    "        driver.quit()\n",
    "        \n",
    "        # Parse the page source with lxml and XPath\n",
    "        tree = html.fromstring(page_source)\n",
    "        \n",
    "        # Extract data using XPath expressions\n",
    "        data_element = tree.xpath('//*[@id=\"shopify-section-template--22415043658042__product-essentials\"]/section/article[2]/div[8]/details[2]/div/div/div[1]/p[4]/text()')\n",
    "        \n",
    "        if data_element:\n",
    "            return ''.join(data_element).strip()\n",
    "        else:\n",
    "            return \"Data element not found on the page.\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "# URL of the webpage you want to scrape\n",
    "url = \"https://www.billabong.com/collections/womens-clothing-tops/products/faye-knit-1\"\n",
    "# Call the function to scrape the website\n",
    "data = scrape_billabong(url)\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49% Cotton, 5% Elastane, 46% Polyester\n"
     ]
    }
   ],
   "source": [
    "def scrape_billabong(url):\n",
    "    try:\n",
    "        # Set Chrome options for headless mode\n",
    "        chrome_options = ChromeOptions()\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "        \n",
    "        # Initialize the WebDriver\n",
    "        driver = Chrome(options=chrome_options)\n",
    "        \n",
    "        # Open the webpage\n",
    "        driver.get(url)\n",
    "        \n",
    "        # Wait for the page to fully load\n",
    "        time.sleep(5)  # Adjust the wait time as needed\n",
    "        \n",
    "        # Get the page source after interactions\n",
    "        page_source = driver.page_source\n",
    "        \n",
    "        # Close the WebDriver once done to avoid multiple instances\n",
    "        driver.quit()\n",
    "        \n",
    "        # Parse the page source with lxml and XPath\n",
    "        tree = html.fromstring(page_source)\n",
    "        \n",
    "        # Extract data using XPath expressions\n",
    "        data_element = tree.xpath('//*[@id=\"shopify-section-template--22415043658042__product-essentials\"]/section/article[2]/div[8]/details[2]/div/div/div[1]/p[4]/text()')\n",
    "        \n",
    "        if data_element:\n",
    "            return ''.join(data_element).strip()\n",
    "        else:\n",
    "            return \"Data element not found on the page.\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "# URL of the webpage you want to scrape\n",
    "url = \"https://www.billabong.com/collections/womens-clothing-tops/products/faye-knit-1\"\n",
    "# Call the function to scrape the website\n",
    "data = scrape_billabong(url)\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% Polyester 5% Elastane. Flat Measurement Not Worn: Length To Hem 43cm/17\\'. Machine Washable. Model Wears US size 6.\n"
     ]
    }
   ],
   "source": [
    "def scrape_boohoo(url):\n",
    "    try:\n",
    "        # Set Chrome options for headless mode\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "        # Initialize the WebDriver\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "        driver.get(url)\n",
    "        # Get the page source after interactions\n",
    "        page_source = driver.page_source\n",
    "        # Close the WebDriver once done to avoid multiple instances\n",
    "        driver.quit()\n",
    "        # Parse the page source with Beautiful Soup\n",
    "        soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "        # Extract data using Beautiful Soup methods\n",
    "        data_element = soup.find(\"p\", class_=\"b-product_details-composition\")\n",
    "        if data_element:\n",
    "            return data_element.text\n",
    "        else:\n",
    "            return \"Data element not found on the page.\"\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "# URL of the webpage you want to scrape\n",
    "url = \"https://us.boohoo.com/lapel-crop-blazer/DZZ52779.html?color=123&_gl=1*m20h85*_up*MQ..*_ga*MjAyMzc4NTAyNi4xNzEzMzk2ODY5*_ga_CKX55DLD7G*MTcxMzM5Njg2OC4xLjEuMTcxMzM5Njk1My4wLjAuMA..\"\n",
    "# Call the function to scrape the website\n",
    "data = scrape_boohoo(url)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fabric: 79% Cotton, 11% Polyester, 10% Viscose, Hotfix: Plastic: Hand Wash Only\n"
     ]
    }
   ],
   "source": [
    "def scrape_nasty_gal(url):\n",
    "    try:\n",
    "        # Set Chrome options for headless mode\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "        # Initialize the WebDriver\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "        driver.get(url)\n",
    "        # Get the page source after interactions\n",
    "        page_source = driver.page_source\n",
    "        # Close the WebDriver once done to avoid multiple instances\n",
    "        driver.quit()\n",
    "        # Parse the page source with Beautiful Soup\n",
    "        soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "        # Extract data using Beautiful Soup methods\n",
    "        data_element = soup.find(\"p\", class_=\"b-product_details-composition\")\n",
    "        if data_element:\n",
    "            return data_element.text\n",
    "        else:\n",
    "            return \"Data element not found on the page.\"\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "# URL of the webpage you want to scrape\n",
    "url = \"https://www.nastygal.com/diamante-denim-wide-leg-jeans/BGG21619.html?_gl=1*dqvdhd*_up*MQ..*_ga*MTU0NzA2MjcuMTcxMzM5NzA0MA..*_ga_YB0PXWCT3D*MTcxMzM5NzAzOS4xLjEuMTcxMzM5NzA1NS4wLjAuMA..\"\n",
    "# Call the function to scrape the website\n",
    "data = scrape_nasty_gal(url)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Style:\n",
      "NF0A88E3\n",
      "Body:\n",
      "230 g/m² 100% Climate Conscious Cotton\n",
      "Rib:\n",
      "290 g/m² 100% Climate Conscious Cotton\n",
      "Sizes:\n",
      "XS, S, M, L, XL, XXL\n",
      "Center Back:\n",
      "20.625''\n",
      "Rib:\n",
      "290 g/m² 100% Climate Conscious Cotton\n"
     ]
    }
   ],
   "source": [
    "def scrape_north_face(url, interactive_element_xpath, loaded_content_xpath, wait_time=10):\n",
    "    try:\n",
    "        # Set Chrome options for headless mode\n",
    "        chrome_options = Options()\n",
    "\n",
    "        # Initialize the WebDriver with headless mode\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "        \n",
    "        # Open the webpage\n",
    "        driver.get(url)\n",
    "\n",
    "        # Wait for the specified time before clicking the interactive element\n",
    "        time.sleep(wait_time)  # Wait for the specified time in seconds\n",
    "\n",
    "        # Find the interactive element\n",
    "        interactive_element = driver.find_element(By.XPATH, interactive_element_xpath)\n",
    "        \n",
    "        # Click the interactive element\n",
    "        interactive_element.click()\n",
    "\n",
    "        # Wait for the loaded content to be visible\n",
    "        loaded_element = WebDriverWait(driver, wait_time).until(\n",
    "            EC.visibility_of_element_located((By.XPATH, loaded_content_xpath))\n",
    "        )\n",
    "\n",
    "        # Once loaded, scrape the content\n",
    "        dynamic_content = loaded_element.text\n",
    "        \n",
    "        return dynamic_content\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None\n",
    "        \n",
    "    finally:\n",
    "        # Close the WebDriver\n",
    "        driver.quit()\n",
    "\n",
    "# Example usage:\n",
    "url = \"https://www.thenorthface.com/en-us/womens/womens-tops/womens-t-shirts-c213341/womens-short-sleeve-heavyweight-tee-pNF0A88E3?color=LK5\"\n",
    "interactive_element_xpath = '//*[@id=\"template-pdp-enabled\"]/div[3]/div/div[4]/div/div/div[2]/div[13]/div[3]/div/button'\n",
    "loaded_content_xpath = '//*[@id=\"tab-panel-name-details-content\"]'\n",
    "\n",
    "dynamic_content = scrape_north_face(url, interactive_element_xpath, loaded_content_xpath)\n",
    "if dynamic_content:\n",
    "    print(dynamic_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5” Inseam Length (Size Small), Relaxed Fit, Elastic Waistband With Logo, Placket Front Center With Interior Button\n",
      "Amina Is Size 2, 34b, And 5’ 8\" (173 Cm), Wearing Skims S\n",
      "47% Cotton / 48% Modal / 5% Spandex\n",
      "Machine Wash Cold, Non-Chlorine Bleach, Tumble Dry Low, Cool Iron, Do Not Dry Clean\n",
      "Imported\n"
     ]
    }
   ],
   "source": [
    "def scrape_north_face(url, interactive_element_xpath, loaded_content_xpath, wait_time=10):\n",
    "    try:\n",
    "        # Set Chrome options for headless mode\n",
    "        chrome_options = Options()\n",
    "\n",
    "        # Initialize the WebDriver with headless mode\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "        \n",
    "        # Open the webpage\n",
    "        driver.get(url)\n",
    "\n",
    "        # Wait for the specified time before clicking the interactive element\n",
    "        time.sleep(wait_time)  # Wait for the specified time in seconds\n",
    "\n",
    "        # Find the interactive element\n",
    "        interactive_element = driver.find_element(By.XPATH, interactive_element_xpath)\n",
    "\n",
    "        time.sleep(wait_time)\n",
    "        \n",
    "        # Click the interactive element\n",
    "        interactive_element.click()\n",
    "\n",
    "        # Wait for the loaded content to be visible\n",
    "        loaded_element = WebDriverWait(driver, wait_time).until(\n",
    "            EC.visibility_of_element_located((By.XPATH, loaded_content_xpath))\n",
    "        )\n",
    "\n",
    "        # Once loaded, scrape the content\n",
    "        dynamic_content = loaded_element.text\n",
    "        \n",
    "        return dynamic_content\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None\n",
    "        \n",
    "    finally:\n",
    "        # Close the WebDriver\n",
    "        driver.quit()\n",
    "\n",
    "# Example usage:\n",
    "url = \"https://skims.com/products/boyfriend-loose-boxer-lily\"\n",
    "interactive_element_xpath = '//*[@id=\"essential\"]/div[3]/div/div[6]/div[2]/button'\n",
    "loaded_content_xpath = '//*[@id=\"essential\"]/div[3]/div/div[6]/div[2]/div/div/div[1]/div/ul'\n",
    "\n",
    "dynamic_content = scrape_north_face(url, interactive_element_xpath, loaded_content_xpath)\n",
    "if dynamic_content:\n",
    "    print(dynamic_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.5% Organic Cotton, 1.5% Elastane\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def scrape_good_american(url):\n",
    "    try:\n",
    "        # Set Chrome options for headless mode\n",
    "        chrome_options = ChromeOptions()\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "        \n",
    "        # Initialize the WebDriver\n",
    "        driver = Chrome(options=chrome_options)\n",
    "        \n",
    "        # Open the webpage\n",
    "        driver.get(url)\n",
    "        \n",
    "        # Wait for the page to fully load\n",
    "        time.sleep(5)  # Adjust the wait time as needed\n",
    "        \n",
    "        # Get the page source after interactions\n",
    "        page_source = driver.page_source\n",
    "        \n",
    "        # Close the WebDriver once done to avoid multiple instances\n",
    "        driver.quit()\n",
    "        \n",
    "        # Parse the page source with lxml and XPath\n",
    "        tree = html.fromstring(page_source)\n",
    "        \n",
    "        # Extract data using XPath expressions\n",
    "        data_element = tree.xpath('//li/strong[text()=\"Fabric:\"]/following-sibling::text()')\n",
    "        \n",
    "        if data_element:\n",
    "            return ''.join(data_element).strip()\n",
    "        else:\n",
    "            return \"Data element not found on the page.\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "# URL of the webpage you want to scrape\n",
    "url = \"https://www.goodamerican.com/products/good-legs-deep-v-clean-hem-blue609?Inseam=Regular\"\n",
    "# Call the function to scrape the website\n",
    "data = scrape_good_american(url)\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pocket Bag:80% Polyester, 20% Cotton / Body:100% Cotton\n"
     ]
    }
   ],
   "source": [
    "def scrape_acne_studios(url):\n",
    "    try:\n",
    "        # Set Chrome options for headless mode\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "        # Initialize the WebDriver\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "        driver.get(url)\n",
    "        # Get the page source after interactions\n",
    "        page_source = driver.page_source\n",
    "        # Close the WebDriver once done to avoid multiple instances\n",
    "        driver.quit()\n",
    "        # Parse the page source with Beautiful Soup\n",
    "        soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "        # Extract data using Beautiful Soup methods\n",
    "        data_element = soup.find(\"h4\", class_=\"h4 fabric-care-mfe__label\")\n",
    "        if data_element:\n",
    "            return data_element.text\n",
    "        else:\n",
    "            return \"Data element not found on the page.\"\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "# URL of the webpage you want to scrape\n",
    "url = \"https://www.hollisterco.com/shop/us/p/low-rise-dark-wash-baggy-jeans-56195470?categoryId=12552&faceout=model&seq=03\"\n",
    "# Call the function to scrape the website\n",
    "data = scrape_acne_studios(url)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96% wool\n",
      "                                \n",
      "\n",
      "                                    4% elastane\n",
      "                                \n",
      "\n",
      "                                    Finished Garment Manufacturing Country/Region:\n",
      "                                    \n",
      "                                        China\n",
      "                                    \n",
      "                                \n",
      "\n",
      "                                    Fabric Production Country/Region:\n",
      "                                    \n",
      "                                        Italy\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def scrape_theory(url):\n",
    "    try:\n",
    "        # Set Chrome options for headless mode\n",
    "        chrome_options = ChromeOptions()\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "        \n",
    "        # Initialize the WebDriver\n",
    "        driver = Chrome(options=chrome_options)\n",
    "        \n",
    "        # Open the webpage\n",
    "        driver.get(url)\n",
    "        \n",
    "        # Wait for the page to fully load\n",
    "        time.sleep(5)  # Adjust the wait time as needed\n",
    "        \n",
    "        # Get the page source after interactions\n",
    "        page_source = driver.page_source\n",
    "        \n",
    "        # Close the WebDriver once done to avoid multiple instances\n",
    "        driver.quit()\n",
    "        \n",
    "        # Parse the page source with lxml and XPath\n",
    "        tree = html.fromstring(page_source)\n",
    "        \n",
    "        # Extract data using XPath expressions\n",
    "        data_element = tree.xpath(\"//ul[@class='list-style-dashed']/li/text()\")\n",
    "        \n",
    "        if data_element:\n",
    "            return '\\n'.join(data_element).strip()\n",
    "        else:\n",
    "            return \"Data element not found on the page.\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "# URL of the webpage you want to scrape\n",
    "url = \"https://www.theory.com/treeca-pant-in-good-wool/H0101234_EA2.html\"\n",
    "# Call the function to scrape the website\n",
    "data = scrape_theory(url)\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Composition: 100%  linen. Lining: 100%  cotton. Sleeve lining: 100%  polyester. Piping: 100%  polyester\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def scrape_theory(url):\n",
    "    try:\n",
    "        # Set Chrome options for headless mode\n",
    "        chrome_options = ChromeOptions()\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "        \n",
    "        # Initialize the WebDriver\n",
    "        driver = Chrome(options=chrome_options)\n",
    "        \n",
    "        # Open the webpage\n",
    "        driver.get(url)\n",
    "        \n",
    "        # Wait for the page to fully load\n",
    "        time.sleep(5)  # Adjust the wait time as needed\n",
    "        \n",
    "        # Get the page source after interactions\n",
    "        page_source = driver.page_source\n",
    "        \n",
    "        # Close the WebDriver once done to avoid multiple instances\n",
    "        driver.quit()\n",
    "        \n",
    "        # Parse the page source with lxml and XPath\n",
    "        tree = html.fromstring(page_source)\n",
    "        \n",
    "        # Extract data using XPath expressions\n",
    "        data_element = tree.xpath('//*[@id=\"productDesktop\"]/main/div/div[3]/div[1]/div[2]/div/p[1]/text()')\n",
    "        \n",
    "        if data_element:\n",
    "            return '\\n'.join(data_element).strip()\n",
    "        else:\n",
    "            return \"Data element not found on the page.\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "# URL of the webpage you want to scrape\n",
    "url = \"https://shop.mango.com/us/women/blazers-blazers/100-linen-suit-blazer_67077112.html?c=99\"\n",
    "# Call the function to scrape the website\n",
    "data = scrape_theory(url)\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13-oz 100% polyester (85% recycled) bonded ¼\"-pile fleece with a windproof mesh backer\n",
      "4.5-oz 100% recycled nylon faille with a DWR (durable water repellent) finish\n",
      "2.1-oz 100% recycled polyester tricot mesh\n",
      "Machine Wash Warm, Do Not Bleach, Tumble Dry Low, Do Not Iron\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def scrape_patagonia(url):\n",
    "    try:\n",
    "        # Set Chrome options for headless mode\n",
    "        chrome_options = ChromeOptions()\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "        \n",
    "        # Initialize the WebDriver\n",
    "        driver = Chrome(options=chrome_options)\n",
    "        \n",
    "        # Open the webpage\n",
    "        driver.get(url)\n",
    "        \n",
    "        # Wait for the page to fully load\n",
    "        time.sleep(5)  # Adjust the wait time as needed\n",
    "        \n",
    "        # Get the page source after interactions\n",
    "        page_source = driver.page_source\n",
    "        \n",
    "        # Close the WebDriver once done to avoid multiple instances\n",
    "        driver.quit()\n",
    "        \n",
    "        # Parse the page source with lxml and XPath\n",
    "        tree = html.fromstring(page_source)\n",
    "        \n",
    "        # Extract data using XPath expressions\n",
    "        data_element = tree.xpath(\"//li[@class='pdp__content-feature']/p[@class='content-feature__description']/text()\")\n",
    "        \n",
    "        if data_element:\n",
    "            return '\\n'.join(data_element).strip()\n",
    "        else:\n",
    "            return \"Data element not found on the page.\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "# URL of the webpage you want to scrape\n",
    "url = \"https://www.patagonia.com/product/mens-classic-retro-x-fleece-jacket/195699845640.html?s_kwcid=17928&utm_source=google&utm_medium=cpc&utm_campaign=BB_Ecomm_Shopping_ALL_WBSP_SaleKWs&gad_source=1&gclid=CjwKCAjw5v2wBhBrEiwAXDDoJd8XQGUW_sof8fEfoDRQUdlBPIlrxyB6fyngvB0--73eUmTbzjhBZBoCa84QAvD_BwE\"\n",
    "# Call the function to scrape the website\n",
    "data = scrape_patagonia(url)\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Takeaway: Keeps it cool and casual\n",
      "An effortlessly cute and comfy staple that pairs perfectly with all your favorite cargos, leggings, and more.\n",
      "Body: 76% Cotton, 24% Polyester\n",
      "Exclusive of Decoration\n"
     ]
    }
   ],
   "source": [
    "def scrape_victorias_secret(url):\n",
    "    try:\n",
    "        # Set Chrome options for headless mode\n",
    "        chrome_options = ChromeOptions()\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "        \n",
    "        # Initialize the WebDriver\n",
    "        driver = Chrome(options=chrome_options)\n",
    "        \n",
    "        # Open the webpage\n",
    "        driver.get(url)\n",
    "        \n",
    "        # Wait for the page to fully load\n",
    "        time.sleep(5)  # Adjust the wait time as needed\n",
    "        \n",
    "        # Get the page source after interactions\n",
    "        page_source = driver.page_source\n",
    "        \n",
    "        # Close the WebDriver once done to avoid multiple instances\n",
    "        driver.quit()\n",
    "        \n",
    "        # Parse the page source with lxml and XPath\n",
    "        tree = html.fromstring(page_source)\n",
    "        \n",
    "        # Extract data using XPath expressions\n",
    "        data_element = tree.xpath(\"//span[@class='prism-danger-zone']/p/text()\")\n",
    "        \n",
    "        if data_element:\n",
    "            return '\\n'.join(data_element).strip()\n",
    "        else:\n",
    "            return \"Data element not found on the page.\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "# URL of the webpage you want to scrape\n",
    "url = \"https://www.victoriassecret.com/us/pink/apparel-catalog/5000009721?brand=pink&collectionId=fe0131b4-96f7-46f6-88a2-3325ba6ed6f5&limit=180&orderBy=REC&priceType=regular&productId=77c4ee8f-ab11-48fa-8991-4e8acd67bb04&stackId=3db62f22-b572-48db-88b2-39a3259392ba&genericId=11243247&choice=3Z3G&product_position=1&stack_position=1&dataSource=manual-collection\"\n",
    "# Call the function to scrape the website\n",
    "data = scrape_victorias_secret(url)\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumpsuits and Playsuits\n",
      "Neckline: Square\n",
      "Sleeve Length: Sleeveless\n",
      "Length: Short\n",
      "Material: 99% Polyester 1% Elastane\n",
      "Fire Safety: WARNING! Keep away from fire\n",
      "Wash before first use. Max temperature 100°F/40°C. Do not bleach. Do not tumble dry. Do not dry clean. Wash similar colors together.; Maximum Temperature 104*F; Do not bleach; Do not tumble dry; Do not dry clean; Wash similar colors together; Wash inside out; Cool iron; Iron on reverse\n",
      "We're happy to refund or exchange any item within 28 days of purchase, if the item is returned in a saleable condition with an original receipt. \n",
      ".\n",
      "HELP\n",
      "USEFUL INFO\n",
      "INSIDE PRIMARK\n",
      "CORPORATE\n"
     ]
    }
   ],
   "source": [
    "def scrape_primark(url):\n",
    "    try:\n",
    "        # Set Chrome options for headless mode\n",
    "        chrome_options = ChromeOptions()\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "        \n",
    "        # Initialize the WebDriver\n",
    "        driver = Chrome(options=chrome_options)\n",
    "        \n",
    "        # Open the webpage\n",
    "        driver.get(url)\n",
    "        \n",
    "        # Wait for the page to fully load\n",
    "        time.sleep(5)  # Adjust the wait time as needed\n",
    "        \n",
    "        # Get the page source after interactions\n",
    "        page_source = driver.page_source\n",
    "        \n",
    "        # Close the WebDriver once done to avoid multiple instances\n",
    "        driver.quit()\n",
    "        \n",
    "        # Parse the page source with lxml and XPath\n",
    "        tree = html.fromstring(page_source)\n",
    "        \n",
    "        # Extract data using XPath expressions\n",
    "        data_element = tree.xpath(\"//p[@class='MuiTypography-root MuiTypography-body1']/text()\")\n",
    "        \n",
    "        if data_element:\n",
    "            return '\\n'.join(data_element).strip()\n",
    "        else:\n",
    "            return \"Data element not found on the page.\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "# URL of the webpage you want to scrape\n",
    "url = \"https://www.primark.com/en-us/p/sleeveless-floral-playsuit-pink-991096807306\"\n",
    "# Call the function to scrape the website\n",
    "data = scrape_primark(url)\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ethical and sustainable T-shirt\n",
      "Unisex sizing & regular cut\n",
      "100% fairly traded organic cotton\n",
      "Good wages for garment workers through our pioneering bonus scheme\n",
      "\n",
      "\n",
      "Solar powered factory\n",
      "185gsm organic cotton\n",
      "Fairtrade & GOTS certified factory\n"
     ]
    }
   ],
   "source": [
    "def scrape_yes_friends(url):\n",
    "    try:\n",
    "        # Set Chrome options for headless mode\n",
    "        chrome_options = ChromeOptions()\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "        \n",
    "        # Initialize the WebDriver\n",
    "        driver = Chrome(options=chrome_options)\n",
    "        \n",
    "        # Open the webpage\n",
    "        driver.get(url)\n",
    "        \n",
    "        # Wait for the page to fully load\n",
    "        time.sleep(5)  # Adjust the wait time as needed\n",
    "        \n",
    "        # Get the page source after interactions\n",
    "        page_source = driver.page_source\n",
    "        \n",
    "        # Close the WebDriver once done to avoid multiple instances\n",
    "        driver.quit()\n",
    "        \n",
    "        # Parse the page source with lxml and XPath\n",
    "        tree = html.fromstring(page_source)\n",
    "        \n",
    "        # Extract data using XPath expressions\n",
    "        data_element = tree.xpath(\"//div[@class='gf_product-desc gf_gs-text-paragraph-1']/ul/li/text()\")\n",
    "        \n",
    "        if data_element:\n",
    "            return '\\n'.join(data_element).strip()\n",
    "        else:\n",
    "            return \"Data element not found on the page.\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "# URL of the webpage you want to scrape\n",
    "url = \"https://yesfriends.co/products/womens-fairtrade-organic-t-shirt?variant=44190218125526\"\n",
    "# Call the function to scrape the website\n",
    "data = scrape_yes_friends(url)\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regular fit\n",
      "Full zip with low collar\n",
      "70% recycled polyester, 30% cotton interlock\n",
      "Front pockets\n",
      "Batwing sleeves\n",
      "Ribbed cuffs and hem\n",
      "FARM Rio graphic print\n",
      "Contains a minimum of 70% recycled and renewable content\n",
      "Imported\n",
      "Product color: Semi Pink Glow / Semi Solar Orange\n",
      "Product code: IQ4497\n"
     ]
    }
   ],
   "source": [
    "## version 1\n",
    "def scrape_adidas(url, interactive_element_xpath, loaded_content_xpath, wait_time=10):\n",
    "    try:\n",
    "        # Set Chrome options for headless mode\n",
    "        chrome_options = Options()\n",
    "\n",
    "        # Initialize the WebDriver with headless mode\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "        \n",
    "        # Open the webpage\n",
    "        driver.get(url)\n",
    "\n",
    "        # Wait for the specified time before clicking the interactive element\n",
    "        time.sleep(wait_time)  # Wait for the specified time in seconds\n",
    "\n",
    "        # Find the interactive element\n",
    "        interactive_element = driver.find_element(By.XPATH, interactive_element_xpath)\n",
    "        \n",
    "        # Click the interactive element\n",
    "        interactive_element.click()\n",
    "\n",
    "        # Wait for the loaded content to be visible\n",
    "        loaded_element = WebDriverWait(driver, wait_time).until(\n",
    "            EC.visibility_of_element_located((By.XPATH, loaded_content_xpath))\n",
    "        )\n",
    "\n",
    "        # Once loaded, scrape the content\n",
    "        dynamic_content = loaded_element.text\n",
    "        \n",
    "        return dynamic_content\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None\n",
    "        \n",
    "    finally:\n",
    "        # Close the WebDriver\n",
    "        driver.quit()\n",
    "\n",
    "# Example usage:\n",
    "url = \"https://www.adidas.com/us/adidas-x-farm-rio-tiro-track-jacket/IQ4497.html\"\n",
    "interactive_element_xpath = '//*[@id=\"navigation-target-specifications\"]/div/button'\n",
    "loaded_content_xpath = '//*[@id=\"navigation-target-specifications\"]/div/div/div/div'\n",
    "\n",
    "dynamic_content = scrape_adidas(url, interactive_element_xpath, loaded_content_xpath)\n",
    "if dynamic_content:\n",
    "    print(dynamic_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slim fit\n",
      "Ribbed crewneck\n",
      "100% recycled polyester tricot\n",
      "AEROREADY\n",
      "Mesh inserts on sides\n",
      "Droptail hem\n",
      "Manchester United woven crest\n",
      "Rose graphic inside back collar\n",
      "Imported\n",
      "Product color: Team Collegiate Red\n",
      "Product code: IP1726\n"
     ]
    }
   ],
   "source": [
    "## version 2\n",
    "def scrape_adidas(url, interactive_element_xpath, loaded_content_xpath, wait_time=20):\n",
    "    try:\n",
    "        user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.71 Safari/537.36\"\n",
    "\n",
    "        # Set Chrome options for headless mode\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(f\"user-agent={user_agent}\")\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "\n",
    "        # Initialize the WebDriver with headless mode\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "        \n",
    "        # Open the webpage\n",
    "        driver.get(url)\n",
    "\n",
    "        # Wait for the interactive element to be visible\n",
    "        WebDriverWait(driver, wait_time).until(\n",
    "            EC.visibility_of_element_located((By.XPATH, interactive_element_xpath))\n",
    "        )\n",
    "\n",
    "        # Find the interactive element\n",
    "        interactive_element = driver.find_element(By.XPATH, interactive_element_xpath)\n",
    "        \n",
    "        # Click the interactive element\n",
    "        interactive_element.click()\n",
    "\n",
    "        # Wait for the loaded content to be visible\n",
    "        loaded_element = WebDriverWait(driver, wait_time).until(\n",
    "            EC.visibility_of_element_located((By.XPATH, loaded_content_xpath))\n",
    "        )\n",
    "\n",
    "        # Once loaded, scrape the content\n",
    "        dynamic_content = loaded_element.text\n",
    "        \n",
    "        return dynamic_content\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None\n",
    "        \n",
    "    finally:\n",
    "        # Close the WebDriver\n",
    "        driver.quit()\n",
    "\n",
    "# Example usage:\n",
    "url = \"https://www.adidas.com/us/manchester-united-23-24-home-jersey/IP1726.html\"\n",
    "interactive_element_xpath = '//*[@id=\"navigation-target-specifications\"]/div/button'\n",
    "loaded_content_xpath = '//*[@id=\"navigation-target-specifications\"]/div/div/div/div'\n",
    "\n",
    "dynamic_content = scrape_adidas(url, interactive_element_xpath, loaded_content_xpath)\n",
    "if dynamic_content:\n",
    "    print(dynamic_content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
