{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from lxml import html\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Links</th>\n",
       "      <th>VideoLink</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NEW IN PRIMARK &amp;amp; TRY ON HAUL + RIVER ISLAN...</td>\n",
       "      <td>https://rstyle.me/+qmS93EYalkwzIXFVo0LSSQ\\nhtt...</td>\n",
       "      <td>https://www.youtube.com/watch?v=6aW6Db_13Zs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Come Shopping With Us üõçÔ∏è | NEW IN H&amp;amp;M, Pri...</td>\n",
       "      <td>https://www.shopLTK.com/explore/elliepearce</td>\n",
       "      <td>https://www.youtube.com/watch?v=x7-iuXE4F9s</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  NEW IN PRIMARK &amp; TRY ON HAUL + RIVER ISLAN...   \n",
       "1  Come Shopping With Us üõçÔ∏è | NEW IN H&amp;M, Pri...   \n",
       "\n",
       "                                               Links  \\\n",
       "0  https://rstyle.me/+qmS93EYalkwzIXFVo0LSSQ\\nhtt...   \n",
       "1        https://www.shopLTK.com/explore/elliepearce   \n",
       "\n",
       "                                     VideoLink  \n",
       "0  https://www.youtube.com/watch?v=6aW6Db_13Zs  \n",
       "1  https://www.youtube.com/watch?v=x7-iuXE4F9s  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../../data/youtube_data/primark_youtube_data.csv')\n",
    "\n",
    "df.head(2)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_primark(url):\n",
    "    try:\n",
    "        # Set Chrome options for headless mode\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "        \n",
    "        # Initialize the WebDriver\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "        \n",
    "        # Open the webpage\n",
    "        driver.get(url)\n",
    "\n",
    "        if 'primark' not in driver.current_url:\n",
    "            return {'item': 'Unsupported URL', 'url': url}\n",
    "        \n",
    "        # Wait for the page to fully load\n",
    "        time.sleep(5)  # Adjust the wait time as needed\n",
    "        \n",
    "        # Get the page source after interactions\n",
    "        page_source = driver.page_source\n",
    "\n",
    "        soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "\n",
    "        # get title\n",
    "        # title = ((soup.find('h1', class_='MuiTypography-root jss1369 MuiTypography-body1')).text).strip()\n",
    "        title_element = soup.find(attrs={\"data-testautomation-id\": \"product-title\"})\n",
    "        title = title_element.text\n",
    "        # print(title_element)\n",
    "        \n",
    "        # Close the WebDriver once done to avoid multiple instances\n",
    "        driver.quit()\n",
    "        \n",
    "        # Parse the page source with lxml and XPath\n",
    "        tree = html.fromstring(page_source)\n",
    "        \n",
    "        # Extract data using XPath expressions\n",
    "        data_element = tree.xpath(\"//p[@class='MuiTypography-root MuiTypography-body1']/text()\")\n",
    "        data_element = ('\\n'.join(data_element).strip()).lower()\n",
    "\n",
    "        # list of materials to match\n",
    "        materials_list = ['cotton', 'recycled cotton', 'organic cotton', 'polyester', 'recycled polyester', 'nylon',\n",
    "            'recycled nylon', 'acrylic', 'spandex', 'elastane', 'flax', 'linen', 'hemp', 'cupro', 'lyocell', 'tencel',\n",
    "            'refibra', 'modal', 'tencel modal', 'viscose', 'bamboo', 'lenzing viscose', 'ecovero', 'silk',\n",
    "            'alpaca', 'wool', 'recycled wool', 'cashmere', 'recycled Cashmere']\n",
    "\n",
    "        # create a regex pattern to match any material from the list\n",
    "        material_pattern = '|'.join(materials_list)\n",
    "\n",
    "        # regex pattern to match the specific percentages and material\n",
    "        pattern = rf'(\\d+)% ({material_pattern})'\n",
    "\n",
    "        fabric_percentages = re.findall(pattern, data_element)\n",
    "        # print(fabric_percentages)\n",
    "    \n",
    "        percentages = [int(material[0]) for material in fabric_percentages]\n",
    "        # make sure percentages sum to 100\n",
    "        if sum(percentages) == 100:\n",
    "            materials = {}\n",
    "\n",
    "            for material_comp in fabric_percentages:\n",
    "                percent = material_comp[0]\n",
    "                fabric = (material_comp[1]).lower()\n",
    "                materials[fabric] = percent\n",
    "\n",
    "                final_data = {'item': title}\n",
    "                final_data.update(materials)\n",
    "\n",
    "        return final_data\n",
    "\n",
    "    except Exception as e:\n",
    "        return {'item': 'No Data', 'url': url}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'item': 'Paula Echevarr√≠a Long Sleeve Crochet Top', 'cotton': '91', 'nylon': '9'}\n",
      "{'item': 'Angel Sleeve Midi Dress', 'viscose': '86', 'nylon': '14'}\n",
      "{'item': 'Lobster Print Swim Shorts', 'polyester': '100'}\n"
     ]
    }
   ],
   "source": [
    "# URL of the webpage you want to scrape\n",
    "urls = [\n",
    "    'https://www.primark.com/en-us/p/paula-echevarria-long-sleeve-crochet-top-cream-991100728116',\n",
    "    'https://www.primark.com/en-us/p/angel-sleeve-midi-dress-blue-991102623505',\n",
    "    'https://www.primark.com/en-us/p/lobster-print-swim-shorts-red-991090592352'\n",
    "]\n",
    "\n",
    "for url in urls:\n",
    "    # Call the function to scrape the website\n",
    "    data = scrape_primark(url)\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_and_update(row):\n",
    "    links_list = row[\"Links\"].split('\\n')\n",
    "    scraped_data_combined = \"\"\n",
    "\n",
    "    for link in links_list:\n",
    "        scraped_data = scrape_primark(link)\n",
    "        scraped_data_combined += str(scraped_data) + \"\\n\"\n",
    "\n",
    "    return scraped_data_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ScrapedData'] = df.apply(scrape_and_update, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('materials_data/primark_materials.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
