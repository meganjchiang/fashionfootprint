{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b44a9753-015a-48f5-8b77-779c6c3166dd",
   "metadata": {},
   "source": [
    "# Pulling data from YouTube using Pagination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2aa6fb-1804-4188-b8b9-9e000a0e2562",
   "metadata": {},
   "source": [
    "### Works Cited\n",
    "This pagination logic was adapted and inspired by the `get_video_ids` function from the INFO 492 Intensive Capstone course, Weeks 1 and 2 lab (`youtube-p1.ipynb`). The function demonstrates efficient retrieval of video IDs using YouTube API's pagination feature, which was instrumental in shaping the approach taken here for fetching and processing fashion-related video data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a70d9fea-db1f-4993-8659-4c6fa84548e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bec6945-c956-43a1-a81e-fd5c52842a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up YouTube Data API \n",
    "api_key = config.API_KEY\n",
    "youtube = build('youtube', 'v3', developerKey=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061cef29-e2ae-46d7-aa07-f442191747d3",
   "metadata": {},
   "source": [
    "### Making my YouTube Data Pull Code Reusable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e484add-43af-4d9c-b78e-988d5e102338",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_youtube_data(brand_name):\n",
    "    published_after = datetime(2023, 9, 1).isoformat() + 'Z'\n",
    "    published_before = datetime(2024, 4, 24).isoformat() + 'Z'\n",
    "    keywords = ['haul', 'clothing', 'clothes', 'shop', 'shopping', 'try on', 'try-on', 'review', 'styling']\n",
    "    social_media_links = ['pinterest', 'youtube', 'twitter', 'instagram', 'tiktok',\n",
    "                          'reddit', 'twitch', 'facebook', 'thmatc', 'spotify']\n",
    "\n",
    "    # fetch initial search results\n",
    "    search_results = []\n",
    "    request = youtube.search().list(\n",
    "        q=brand_name,\n",
    "        part='snippet',\n",
    "        type='video',\n",
    "        publishedAfter=published_after,\n",
    "        publishedBefore=published_before,\n",
    "        maxResults=50\n",
    "    )\n",
    "    response = request.execute()\n",
    "    next_page_token = response.get('nextPageToken')\n",
    "    \n",
    "    while next_page_token is not None:\n",
    "        # send request to YouTube API\n",
    "        request = youtube.search().list(\n",
    "            q=brand_name,\n",
    "            part='snippet',\n",
    "            type='video',\n",
    "            publishedAfter=published_after,\n",
    "            publishedBefore=published_before,\n",
    "            maxResults=50,\n",
    "            pageToken=next_page_token\n",
    "        )\n",
    "        response = request.execute()\n",
    "        # add items from response \n",
    "        search_results.extend(response.get('items',[]))\n",
    "        # get next page token for pagination\n",
    "        next_page_token = response.get('nextPageToken')\n",
    "\n",
    "    # process search results to extract relevant vid data\n",
    "    brand_videos = []\n",
    "    item_link_pattern = re.compile(r'(.+?) - (?:\\$(\\d+)\\n)?(https?://\\S+)')\n",
    "\n",
    "    for search_result in search_results:\n",
    "        # gets and stores video id\n",
    "        video_id = search_result['id']['videoId']\n",
    "        video_response = youtube.videos().list(\n",
    "            # receive snippet part of data - title, description, tags, etc.\n",
    "            part=\"snippet\",\n",
    "            id=video_id\n",
    "        ).execute()\n",
    "\n",
    "        # access description field of snipper\n",
    "        description = video_response['items'][0]['snippet']['description']\n",
    "        # extract links from description\n",
    "        links = re.findall(r'(https?://\\S+)', description)\n",
    "        # makes all titles lowercase so code can match on any version of title:\n",
    "        title = search_result['snippet']['title'].lower()\n",
    "\n",
    "        # filters based on brand name and fashion related keywords\n",
    "        if brand_name.lower() in title and any(keyword in title for keyword in keywords):\n",
    "            # filters out social media links\n",
    "            # filtered_links = [link for link in links if not any(keyword in link for keyword in social_media_links)]\n",
    "            # match on regex pattern instead of filtering out other stuff...\n",
    "            matches = item_link_pattern.findall(description)\n",
    "            item_details = [{'name': match[0], 'link': match[1]} for match in matches]\n",
    "            # get link to video\n",
    "            video_link = f\"https://www.youtube.com/watch?v={video_id}\"\n",
    "            brand_videos.append({\n",
    "                'title': search_result['snippet']['title'],\n",
    "                'links': item_details, \n",
    "                'videoLink': video_link\n",
    "            })\n",
    "\n",
    "    # process data and format for csv\n",
    "    brand_youtube_data = []\n",
    "    for video in brand_videos:\n",
    "        if video['links']:\n",
    "            brand_youtube_data.append({\n",
    "                'Title': video['title'],\n",
    "                'Links': '\\n'.join(video['links']),\n",
    "                'VideoLink': video['videoLink']\n",
    "            })\n",
    "\n",
    "    return brand_youtube_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0c3167a-3299-421c-a96a-e3e2391e73b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, dict found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m brand \u001b[38;5;129;01min\u001b[39;00m brands:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# start timing\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 7\u001b[0m     youtube_data \u001b[38;5;241m=\u001b[39m get_youtube_data(brand)\n\u001b[1;32m      8\u001b[0m     all_brand_youtube_data[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbrand\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_youtube_data\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m youtube_data\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# save data to csv files\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 79\u001b[0m, in \u001b[0;36mget_youtube_data\u001b[0;34m(brand_name)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m video \u001b[38;5;129;01min\u001b[39;00m brand_videos:\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m video[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinks\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     77\u001b[0m         brand_youtube_data\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     78\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTitle\u001b[39m\u001b[38;5;124m'\u001b[39m: video[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m---> 79\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLinks\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(video[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinks\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[1;32m     80\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVideoLink\u001b[39m\u001b[38;5;124m'\u001b[39m: video[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvideoLink\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     81\u001b[0m         })\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m brand_youtube_data\n",
      "\u001b[0;31mTypeError\u001b[0m: sequence item 0: expected str instance, dict found"
     ]
    }
   ],
   "source": [
    "brands = ['Princess Polly', 'Mango']\n",
    "all_brand_youtube_data = {}\n",
    "\n",
    "for brand in brands:\n",
    "    # start timing\n",
    "    start_time = time.time()\n",
    "    youtube_data = get_youtube_data(brand)\n",
    "    all_brand_youtube_data[f'{brand.lower().replace(\" \", \"_\")}_youtube_data'] = youtube_data\n",
    "    # save data to csv files\n",
    "    file_name = f'{brand.lower().replace(\" \", \"_\")}_youtube_data'\n",
    "    filename = f\"../data/youtube_data/{file_name}.csv\"\n",
    "    pd.DataFrame(youtube_data).to_csv(filename, index=False)\n",
    "    # calculate time\n",
    "    elapsed_time = time.time() - start_time  \n",
    "    print(f\"CSV file saved: {filename} (runtime: {elapsed_time:.2f} seconds)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a769906-bb1e-498e-aa3a-2e1542a15124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of brands\n",
    "brands = ['Adidas', 'Nike', 'Sandy Liang']\n",
    "\n",
    "# done\n",
    "# 'Aritzia', 'Skims', 'Shein', 'Princess Polly'\n",
    "# 'Abercrombie & Fitch', 'Abercrombie and Fitch', 'Abercrombie', 'Amazon'\n",
    "# 'Brandy', 'Brandy Melville', 'Uniqlo'\n",
    "# 'Alo Yoga', 'Alo', 'Reformation'\n",
    "# 'Boohoo', 'Nasty Gal', 'Patagonia', 'Hollister'\n",
    "# 'VS', 'Victoria\\'s Secret', 'PINK', 'Victorias Secret'\n",
    "# 'Alice + Olivia', 'Alice & Olivia', 'Alica and Olivia', 'Billabong'\n",
    "# 'Primark', 'Yes Friends', 'ASOS', 'Forever 21'\n",
    "# 'Adidas', 'Nike', 'Sandy Liang'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb578698-1122-48e6-a8de-6e849ecc77af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict to store YouTube data for all brands\n",
    "# keys: brand names, values: data from YouTube\n",
    "all_brand_youtube_data = {}\n",
    "\n",
    "# iterate over each brand and fetch and process data\n",
    "for brand in brands:\n",
    "    youtube_data = get_youtube_data(brand)\n",
    "    # creates key for brand's data and assigns YouTube data to it to add to dict\n",
    "    file_name = f'{brand.lower().replace(\" \", \"_\")}_youtube_data'\n",
    "    all_brand_youtube_data[file_name] = youtube_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "265d2d72-62c7-41f4-95cc-e2fc6f29043b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file saved: ../data/youtube_data/adidas_youtube_data.csv\n",
      "CSV file saved: ../data/youtube_data/nike_youtube_data.csv\n",
      "CSV file saved: ../data/youtube_data/sandy_liang_youtube_data.csv\n"
     ]
    }
   ],
   "source": [
    "# save data to csv files (in youtube_data folder in the data folder)\n",
    "for name, data in all_brand_youtube_data.items():\n",
    "    filename = f\"../data/youtube_data/{name}.csv\"\n",
    "    pd.DataFrame(data).to_csv(filename, index=False)\n",
    "    print(f\"CSV file saved: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbae562e-27b0-4fa7-ad8b-c2a388f2b3ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
