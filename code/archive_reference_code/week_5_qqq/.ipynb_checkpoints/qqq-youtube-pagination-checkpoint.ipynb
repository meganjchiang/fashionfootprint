{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f56f2eea-149a-4ba7-b121-3b833d09022a",
   "metadata": {},
   "source": [
    "# FashionFootprint QQQ Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381cad3e-4735-4dbf-8860-749d7b8bbe41",
   "metadata": {},
   "source": [
    "## **Q9**  How can we make data collection more efficient? *(Salley)*\n",
    "\n",
    "### Qualitative:\n",
    "#### Problem - \n",
    "- In last week's YouTube data pull, I had to rewrite code for each brand (long, tedious, more difficult to read)\n",
    "- I also was limited to the first 50 results\n",
    "#### Hypothesis & Claim - \n",
    "- How can I incorporate the pagination methods we learned in Week 1/2's lab in pulling data?\n",
    "- I should be able to adjust my code to follow a similar pagination method to the lab and get more results.\n",
    "#### Context, Motivation & Rationale - \n",
    "- We want larger datasets\n",
    "- We want more efficient and more readable code\n",
    "- Many of the videos in datasets we pulled last week did not include links we could scrape from, so having more data could allow us to build a bigger dataset\n",
    "#### Assumptions & Biases - \n",
    "- Assuming all data gathered is accurate\n",
    "- Biases towards certain keywords that could limit the types of fashion videos we pull\n",
    "#### Definitions, Data, and Methods - \n",
    "- Data from YouTube API\n",
    "- From `youtube-p1.ipynb` - *Your Final Task for Part 1 - Make this Reusable* code\n",
    "- **Pagination** - retrieving large number of search results from YouTube API using multiple requests rather than getting them all in a single request\n",
    "   - More efficient to handle larger datasets in smaller chunks\n",
    "   - Ensure retrieval all the relevant search results for each brand by making multiple requests to YouTube API\n",
    "   - Each request fetches a new page of results until all results have been retrieved\n",
    "- **Methods:**\n",
    "   - Using `nextPageToken` to use in next request to get next page of results\n",
    "   - To retrieve all results for query, make multiple requests by passing `nextPageToken` received from previous request until there are no more results available"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74beaf0-0b45-407b-9e04-6fa852e91047",
   "metadata": {},
   "source": [
    "### Quantitative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0eb0760a-2cbb-495d-b070-7228b04bdcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "import os\n",
    "import config\n",
    "\n",
    "# set up YouTube Data API \n",
    "api_key = config.API_KEY\n",
    "youtube = build('youtube', 'v3', developerKey=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0283b07-1463-4ac6-b208-180d6644308c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_youtube_data(brand_name):\n",
    "    published_after = datetime(2023, 9, 1).isoformat() + 'Z'\n",
    "    published_before = datetime(2024, 4, 24).isoformat() + 'Z'\n",
    "    keywords = ['haul', 'clothing', 'clothes', 'shop', 'shopping', 'try on', 'try-on', 'review', 'styling']\n",
    "    social_media_links = ['pinterest', 'youtube', 'twitter', 'instagram', 'tiktok',\n",
    "                          'reddit', 'twitch', 'facebook', 'thmatc', 'spotify']\n",
    "\n",
    "    # fetch initial search results\n",
    "    search_results = []\n",
    "    request = youtube.search().list(\n",
    "        q=brand_name,\n",
    "        part='snippet',\n",
    "        type='video',\n",
    "        publishedAfter=published_after,\n",
    "        publishedBefore=published_before,\n",
    "        maxResults=10\n",
    "    )\n",
    "    response = request.execute()\n",
    "    next_page_token = response.get('nextPageToken')\n",
    "    \n",
    "    while next_page_token is not None:\n",
    "        # send request to YouTube API\n",
    "        request = youtube.search().list(\n",
    "            q=brand_name,\n",
    "            part='snippet',\n",
    "            type='video',\n",
    "            publishedAfter=published_after,\n",
    "            publishedBefore=published_before,\n",
    "            maxResults=5,\n",
    "            pageToken=next_page_token\n",
    "        )\n",
    "        response = request.execute()\n",
    "        # add items from response \n",
    "        search_results.extend(response.get('items',[]))\n",
    "        # get next page token for pagination\n",
    "        next_page_token = response.get('nextPageToken')\n",
    "\n",
    "    # process search results to extract relevant vid data\n",
    "    brand_videos = []\n",
    "    for search_result in search_results:\n",
    "        # gets and stores video id\n",
    "        video_id = search_result['id']['videoId']\n",
    "        video_response = youtube.videos().list(\n",
    "            # receive snippet part of data - title, description, tags, etc.\n",
    "            part=\"snippet\",\n",
    "            id=video_id\n",
    "        ).execute()\n",
    "\n",
    "        # access description field of snipper\n",
    "        description = video_response['items'][0]['snippet']['description']\n",
    "        # extract links from description\n",
    "        links = re.findall(r'(https?://\\S+)', description)\n",
    "        # makes all titles lowercase so code can match on any version of title:\n",
    "        title = search_result['snippet']['title'].lower()\n",
    "\n",
    "        # filters based on brand name and fashion related keywords\n",
    "        if brand_name.lower() in title and any(keyword in title for keyword in keywords):\n",
    "            # filters out social media links\n",
    "            filtered_links = [link for link in links if not any(keyword in link for keyword in social_media_links)]\n",
    "            # get link to video\n",
    "            video_link = f\"https://www.youtube.com/watch?v={video_id}\"\n",
    "            brand_videos.append({\n",
    "                'title': search_result['snippet']['title'],\n",
    "                'links': filtered_links,\n",
    "                'videoLink': video_link\n",
    "            })\n",
    "\n",
    "    # process data and format for csv\n",
    "    brand_youtube_data = []\n",
    "    for video in brand_videos:\n",
    "        if video['links']:\n",
    "            brand_youtube_data.append({\n",
    "                'Title': video['title'],\n",
    "                'Links': '\\n'.join(video['links']),\n",
    "                'VideoLink': video['videoLink']\n",
    "            })\n",
    "\n",
    "    return brand_youtube_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e7cc418-62a7-463b-9714-c7ffdce854db",
   "metadata": {},
   "outputs": [
    {
     "ename": "HttpError",
     "evalue": "<HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/search?q=Uniqlo&part=snippet&type=video&publishedAfter=2023-09-01T00%3A00%3A00Z&publishedBefore=2024-04-24T00%3A00%3A00Z&maxResults=10&key=AIzaSyCs8DUh5dGPv0ZlKZx20k38TrQmhsuw4yM&alt=json returned \"The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.\". Details: \"[{'message': 'The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.', 'domain': 'youtube.quota', 'reason': 'quotaExceeded'}]\">",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHttpError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m youtube_data \u001b[38;5;241m=\u001b[39m get_youtube_data(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUniqlo\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m video \u001b[38;5;129;01min\u001b[39;00m youtube_data:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTitle: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvideo[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTitle\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 18\u001b[0m, in \u001b[0;36mget_youtube_data\u001b[0;34m(brand_name)\u001b[0m\n\u001b[1;32m      9\u001b[0m search_results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     10\u001b[0m request \u001b[38;5;241m=\u001b[39m youtube\u001b[38;5;241m.\u001b[39msearch()\u001b[38;5;241m.\u001b[39mlist(\n\u001b[1;32m     11\u001b[0m     q\u001b[38;5;241m=\u001b[39mbrand_name,\n\u001b[1;32m     12\u001b[0m     part\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msnippet\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m     maxResults\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m\n\u001b[1;32m     17\u001b[0m )\n\u001b[0;32m---> 18\u001b[0m response \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39mexecute()\n\u001b[1;32m     19\u001b[0m next_page_token \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnextPageToken\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m next_page_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# send request to YouTube API\u001b[39;00m\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.11/site-packages/googleapiclient/_helpers.py:130\u001b[0m, in \u001b[0;36mpositional.<locals>.positional_decorator.<locals>.positional_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m positional_parameters_enforcement \u001b[38;5;241m==\u001b[39m POSITIONAL_WARNING:\n\u001b[1;32m    129\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.11/site-packages/googleapiclient/http.py:938\u001b[0m, in \u001b[0;36mHttpRequest.execute\u001b[0;34m(self, http, num_retries)\u001b[0m\n\u001b[1;32m    936\u001b[0m     callback(resp)\n\u001b[1;32m    937\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[0;32m--> 938\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HttpError(resp, content, uri\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muri)\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostproc(resp, content)\n",
      "\u001b[0;31mHttpError\u001b[0m: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/search?q=Uniqlo&part=snippet&type=video&publishedAfter=2023-09-01T00%3A00%3A00Z&publishedBefore=2024-04-24T00%3A00%3A00Z&maxResults=10&key=AIzaSyCs8DUh5dGPv0ZlKZx20k38TrQmhsuw4yM&alt=json returned \"The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.\". Details: \"[{'message': 'The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.', 'domain': 'youtube.quota', 'reason': 'quotaExceeded'}]\">"
     ]
    }
   ],
   "source": [
    "youtube_data = get_youtube_data('Uniqlo')\n",
    "\n",
    "for video in youtube_data:\n",
    "    print(f\"Title: {video['Title']}\")\n",
    "    print(f\"Links: {video['Links']}\")\n",
    "    print(f\"Video Link: {video['VideoLink']}\")\n",
    "    print()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "890e2fe2-e9f3-4454-a75d-87eb2b5f0de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of Uniqlo YouTube data:\n",
      "                                               Title  \\\n",
      "0  vlog | tips on layering jewellery, Uniqlo haul...   \n",
      "1  HUGE Uniqlo Try-On Haul ! Should I Keep or Ret...   \n",
      "2  UNIQLO Pants| Are UNIQLO Pants Quality? Practi...   \n",
      "3                      Uniqlo: C Try-On &amp; Review   \n",
      "4  UNIQLO WINTER CAPSULE TRY ON HAUL! MY FAVE UNI...   \n",
      "\n",
      "                                               Links  \\\n",
      "0  https://to.pandora.net/iamcharlotteolivia-cher...   \n",
      "1  https://www.uniqlo.com/us/en/products/E450606-...   \n",
      "2  https://www.uniqlo.com/us/en/products/E464744-...   \n",
      "3  https://shopmy.us/cecifunnce\\nhttps://go.shopm...   \n",
      "4  http://loveindiamoon.co.uk\\nhttps://www.vinted...   \n",
      "\n",
      "                                     VideoLink  \n",
      "0  https://www.youtube.com/watch?v=dXUSSkPrvEw  \n",
      "1  https://www.youtube.com/watch?v=DS86919P3Vg  \n",
      "2  https://www.youtube.com/watch?v=gNLCIBOomjM  \n",
      "3  https://www.youtube.com/watch?v=aOXPQlC3aw8  \n",
      "4  https://www.youtube.com/watch?v=jChlH6uDrmo  \n"
     ]
    }
   ],
   "source": [
    "# reads uniqlo CSV into a pandas df\n",
    "uniqlo_df = pd.read_csv(\"../data/youtube_data/uniqlo_youtube_data.csv\")\n",
    "\n",
    "# display the first 5 rows\n",
    "print(\"First 5 rows of Uniqlo YouTube data:\")\n",
    "print(uniqlo_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce0f0ed-3b54-4070-9ae1-687299fc7fb5",
   "metadata": {},
   "source": [
    "### Qualitative (pt. 2):\n",
    "#### Answer/Update to Question/Claim\n",
    "- Was able to use pagination for my code!\n",
    "- Did not speed up data collection (for a list of brands of 3-4, it takes ~3 minutes)\n",
    "- Made the code file much shorter since I could re-use code\n",
    "- Made datasets much longer\n",
    "#### Summary & Re-contextualization\n",
    "- I was able to use similar methods as the YouTube lab to make my data pull processes from Week 4 reusable\n",
    "- The code still runs for a long time and I have to use small lists of brands, but it makes datasets much longer\n",
    "#### Story & Domain Knowledge\n",
    "- Learned about limitations of YouTube API and API keys in general\n",
    "- Learned about pagination and how it can increase efficiency and amount of data gathered\n",
    "#### Uncertainty, Limitations & Caveats\n",
    "- Reach quota limits very quickly\n",
    "- Max number of brands I am able to pull from each API key is 4 brands\n",
    "- Takes around 3 minutes to run each time\n",
    "- Still tedious and time consuming, but able to collect a lot more data\n",
    "#### New Problems & Next Steps\n",
    "- How do we display this data in our Chrome Extension?\n",
    "- How can we combine this with web scraping? Can we combine it?\n",
    "- How do we combine datasets to access the correct item and its corresponding scores?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03288b20-060e-4ce9-8e89-18458aa4468c",
   "metadata": {},
   "source": [
    "## **Q10**  How do we connect outputting scores & other information with YouTube data? *(Salley)*\n",
    "\n",
    "### Qualitative:\n",
    "#### Problem - \n",
    "- How do we connect different aspects of what our team is doing to create the Chrome extension?\n",
    "- How can we combine our CSV files and scores?\n",
    "#### Hypothesis & Claim - \n",
    "- We should be able to connect CSVs together by matching by brands, titles, and links\n",
    "- Based on a link that we read, we can display information about clothing items in the video's description\n",
    "#### Context, Motivation & Rationale - \n",
    "- Because we have been able to read links using web scraping, I am growing optimistic of how we can use a Chrome Extension to read the link of the YouTube video a person is watching and matching it with a link and/or title from our dataset\n",
    "- I want to match by a certain factor to both put CSVs together and connect user input to items in our dataset\n",
    "#### Assumptions & Biases - \n",
    "- Assuming YouTube links that we pull from YouTube API link to real videos\n",
    "- I may have bias towards certain keywords and brands that makes me prioritize them during the testing and building process\n",
    "#### Definitions, Data, and Methods - \n",
    "- Using Google Sheets to combine our CSVs together\n",
    "   - Used `brand_info.csv` to pull people, planet, and brand score\n",
    "   - Used *selflessclothes.com* to pull material score\n",
    "- Converted combined CSV to JSON using ChatGPT\n",
    "- Created basic web page using JS & HTML that took in a link, matched link with link in dataset, and outputted item name and scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721a701a-8f88-4ddf-a880-747ad5ca5b97",
   "metadata": {},
   "source": [
    "### Quantitative:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58c5cea-322a-4f69-a1a7-e6fbd98c8f8f",
   "metadata": {},
   "source": [
    "**`index.html`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fef7f3cc-b977-4060-8a24-a79f601e59dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <!DOCTYPE html>\n",
    "# <html lang=\"en\">\n",
    "# <head>\n",
    "#     <meta charset=\"UTF-8\">\n",
    "#     <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "#     <title>Sustainability Score Finder</title>\n",
    "#     <link rel=\"stylesheet\" href=\"style.css\">\n",
    "# </head>\n",
    "# <body>\n",
    "#     <div class=\"video-link-box\">\n",
    "#         <h1>Sustainability Score Finder</h1>\n",
    "#         <input type=\"text\" id=\"videoLinkInput\" placeholder=\"Enter video link here\">\n",
    "#         <button onclick=\"findScores()\" id=\"findButton\">Find Scores!</button>\n",
    "#         <div id=\"result\"></div>\n",
    "#     </div>\n",
    "#     <script src=\"basic-webpage.js\"></script>\n",
    "# </body>\n",
    "# </html>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df92e0c4-61d2-452f-9ee4-c40f97ce3363",
   "metadata": {},
   "source": [
    "**`basic-webpage.js`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d53c9b3-fc00-465b-a7dd-f8cb9b7338bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# async function findScores() {\n",
    "#     // retrieves inputted vid link from input field\n",
    "#     var videoLink = document.getElementById(\"videoLinkInput\").value;\n",
    "\n",
    "#     try {\n",
    "#         // get data from brand json file\n",
    "#         var response = await fetch('uniqlo-data.json');\n",
    "#         var brandDataset = await response.json();\n",
    "        \n",
    "#         // retrieves reference to result element (from html)\n",
    "#         var resultDiv = document.getElementById(\"result\");\n",
    "#         // if link is found...\n",
    "#         var found = false;\n",
    "\n",
    "#         // basic table to display results\n",
    "#         var itemScoreTable = `\n",
    "#             <table>\n",
    "#                 <tr>\n",
    "#                     <th>Item</th>\n",
    "#                     <th>Material Score</th>\n",
    "#                     <th>Overall Score</th>\n",
    "#                     <th>People Score</th>\n",
    "#                     <th>Planet Score</th>\n",
    "#                 </tr>\n",
    "#         `;\n",
    "\n",
    "#         for (var i=0; i < brandDataset.length; i++) {\n",
    "#             // finds matching video link in brand dataset\n",
    "#             // find more efficient way of doing this later...\n",
    "#             if (brandDataset[i].VideoLink === videoLink) {\n",
    "#                 found = true;\n",
    "#                 // retrieves links to items in the vid's description\n",
    "#                 var items = brandDataset[i].Links;\n",
    "#                 // retrieves item name and scores\n",
    "#                 for (var j=0; j < items.length; j++) {\n",
    "#                     var item = items[j].ScrapedData.item;\n",
    "#                     var materialScore = items[j].MaterialScore;\n",
    "#                     var overallScore = items[j].overall_score;\n",
    "#                     var peopleScore = items[j].people_score;\n",
    "#                     var planetScore = items[j].planet_score;\n",
    "#                     // adds each item to table \n",
    "#                     // MAKE LOOK BETTER\n",
    "#                     itemScoreTable += `\n",
    "#                         <tr>\n",
    "#                             <td>${item}</td>\n",
    "#                             <td>${materialScore}</td>\n",
    "#                             <td>${overallScore}</td>\n",
    "#                             <td>${peopleScore}</td>\n",
    "#                             <td>${planetScore}</td>\n",
    "#                         </tr>\n",
    "#                     `;\n",
    "#                 }\n",
    "#             }\n",
    "#         }\n",
    "#         itemScoreTable += `</table>`;\n",
    "\n",
    "#         if (found) {\n",
    "#             resultDiv.innerHTML = itemScoreTable;\n",
    "#         } else {\n",
    "#             resultDiv.innerHTML = \"Video link not found in the brandDataset.\";\n",
    "#         }\n",
    "#     } catch (error) {\n",
    "#         console.error('Error fetching or parsing data:', error);\n",
    "#     }\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b4f4a3-cc49-409a-8e79-fb79b4c9c37c",
   "metadata": {},
   "source": [
    "**`uniqlo-data.json`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8650ca8f-af03-4ae2-9519-8bfce3fdae0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# {\n",
    "#       \"Title\": \"Uniqlo U Spring/Summer 2023 Styling Haul + Monthly Favourites\",\n",
    "#       \"ShortTitle\": \"Uniqlo U Spring/Summer 2023 Styling Haul\",\n",
    "#       \"VideoLink\": \"https://www.youtube.com/watch?v=9hktZEc3Vhs\",\n",
    "#       \"Links\": [\n",
    "#         {\n",
    "#           \"URL\": \"https://rstyle.me/+CZM_fbUHWdR5PT9pvIqK8Q\",\n",
    "#           \"ScrapedData\": {\n",
    "#             \"item\": \"U Oversized Single Breasted Coat\",\n",
    "#             \"cotton\": 35,\n",
    "#             \"nylon\": 30,\n",
    "#             \"polyester\": 65\n",
    "#           },\n",
    "#           \"CleanScrapedData\": {\n",
    "#             \"item\": \"U Oversized Single Breasted Coat\",\n",
    "#             \"cotton\": 70,\n",
    "#             \"nylon\": 30\n",
    "#           },\n",
    "#           \"MaterialScore\": 0.3,\n",
    "#           \"overall_score\": 3,\n",
    "#           \"people_score\": 3,\n",
    "#           \"planet_score\": 3\n",
    "#         },\n",
    "#         {\n",
    "#           \"URL\": \"https://rstyle.me/+MAdavWCUzekigB04gtAKaw\",\n",
    "#           \"ScrapedData\": {\n",
    "#             \"item\": \"U Short Jacket\",\n",
    "#             \"cotton\": 100,\n",
    "#             \"polyester\": 65\n",
    "#           },\n",
    "#           \"CleanScrapedData\": {\n",
    "#             \"item\": \"U Short Jacket\",\n",
    "#             \"cotton\": 100\n",
    "#           },\n",
    "#           \"MaterialScore\": 0.9,\n",
    "#           \"overall_score\": 3,\n",
    "#           \"people_score\": 3,\n",
    "#           \"planet_score\": 3\n",
    "#         }\n",
    "#       ]\n",
    "#     }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604daac5-e4e2-4dfc-9db5-d45cfa9558b3",
   "metadata": {},
   "source": [
    "### Qualitative (pt. 2):\n",
    "#### Answer/Update to Question/Claim\n",
    "- We can use **brand name** and **YouTube link** to connect scores to each other and videos to items in our dataset\n",
    "- Our Chrome Extension could potentially read the link of the video a user is currently on, match it with a link from our datasets to output information about it\n",
    "- We did low-level, manual combining of datasets by looking for matching brands from CSV files and putting it together on Google Sheets \n",
    "#### Summary & Re-contextualization\n",
    "- We were able to combine information across multiple datasets (Salley's YouTube video pull, Jasmine's brand-specific web scraping, Sabrina & Megan's brand/clothing item score web scraping)\n",
    "#### Story & Domain Knowledge\n",
    "- Learned about how to read in json files in JS\n",
    "- Learned about how to look for an item in a dataset based on user input and display other information within that row\n",
    "#### Uncertainty, Limitations & Caveats\n",
    "- Is this possible on a larger scale - we are currently working on a very small json file and matching by using for loops...how possible is this to do on a much larger dataset?\n",
    "- The basic webpage we made is not a Chrome Extension - will it apply to Chrome Extension-building?\n",
    "#### New Problems & Next Steps\n",
    "- How do we want to display scores? In what format (design-focused)\n",
    "- Can we display more information? Can we display brand recommendations?\n",
    "- How do we convert the code we wrote for the web page to a Chrome Extension?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
