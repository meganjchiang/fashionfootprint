{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b44a9753-015a-48f5-8b77-779c6c3166dd",
   "metadata": {},
   "source": [
    "# Pulling data from YouTube using Pagination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2aa6fb-1804-4188-b8b9-9e000a0e2562",
   "metadata": {},
   "source": [
    "### TO DO!\n",
    "- DO GIT PULL AND MERGE BEFORE PUSHING!!!!!!  \n",
    "- ~Try pagination method to get more data for more brands and increase efficiency~\n",
    "- ~Include video links in data pulls~\n",
    "- “using python pandas to read csv and check for how many data rows are present in the file, along with doing other file manipulation task through pandas”  \n",
    "- Format CSV files better\n",
    "- Format as QQQ\n",
    "\n",
    "- SEE IF WE CAN LIMIT COUNTRY OF VIDEO? SEE IF LANGUAGE IS ENGLISH? SO WE CAN GET SMALLER SET OF VIDEOS\n",
    "- manually input material scores for brandy and uniqlo\n",
    "- make basic webpage to input link and get brand ratings and material score\n",
    "- remake csv's using 2024 data\n",
    "- learn HIGG\n",
    "- pull item into dictionary\n",
    "- {item: ' ', material: ' ', "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d8c8d4-2783-462d-a6b1-50bc66087e62",
   "metadata": {},
   "source": [
    "### QUESTIONS!\n",
    "- how to increase quota limit\n",
    "- does doing pagination increase quota usage? i feel like i was able to get more data by using separate code for each brand\n",
    "- is my code efficient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a70d9fea-db1f-4993-8659-4c6fa84548e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "import os\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bec6945-c956-43a1-a81e-fd5c52842a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up YouTube Data API \n",
    "api_key = config.API_KEY\n",
    "youtube = build('youtube', 'v3', developerKey=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061cef29-e2ae-46d7-aa07-f442191747d3",
   "metadata": {},
   "source": [
    "### Making my YouTube Data Pull Code Reusable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e484add-43af-4d9c-b78e-988d5e102338",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_youtube_data(brand_name):\n",
    "    published_after = datetime(2023, 9, 1).isoformat() + 'Z'\n",
    "    published_before = datetime(2024, 4, 24).isoformat() + 'Z'\n",
    "    keywords = ['haul', 'clothing', 'clothes', 'shop', 'shopping', 'try on', 'try-on', 'review', 'styling']\n",
    "    social_media_links = ['pinterest', 'youtube', 'twitter', 'instagram', 'tiktok',\n",
    "                          'reddit', 'twitch', 'facebook', 'thmatc', 'spotify']\n",
    "\n",
    "    # fetch initial search results\n",
    "    search_results = []\n",
    "    request = youtube.search().list(\n",
    "        q=brand_name,\n",
    "        part='snippet',\n",
    "        type='video',\n",
    "        publishedAfter=published_after,\n",
    "        publishedBefore=published_before,\n",
    "        maxResults=50\n",
    "    )\n",
    "    response = request.execute()\n",
    "    next_page_token = response.get('nextPageToken')\n",
    "    \n",
    "    while next_page_token is not None:\n",
    "        # send request to YouTube API\n",
    "        request = youtube.search().list(\n",
    "            q=brand_name,\n",
    "            part='snippet',\n",
    "            type='video',\n",
    "            publishedAfter=published_after,\n",
    "            publishedBefore=published_before,\n",
    "            maxResults=50,\n",
    "            pageToken=next_page_token\n",
    "        )\n",
    "        response = request.execute()\n",
    "        # add items from response \n",
    "        search_results.extend(response.get('items',[]))\n",
    "        # get next page token for pagination\n",
    "        next_page_token = response.get('nextPageToken')\n",
    "\n",
    "    # process search results to extract relevant vid data\n",
    "    brand_videos = []\n",
    "    for search_result in search_results:\n",
    "        # gets and stores video id\n",
    "        video_id = search_result['id']['videoId']\n",
    "        video_response = youtube.videos().list(\n",
    "            # receive snippet part of data - title, description, tags, etc.\n",
    "            part=\"snippet\",\n",
    "            id=video_id\n",
    "        ).execute()\n",
    "\n",
    "        # access description field of snipper\n",
    "        description = video_response['items'][0]['snippet']['description']\n",
    "        # extract links from description\n",
    "        links = re.findall(r'(https?://\\S+)', description)\n",
    "        # makes all titles lowercase so code can match on any version of title:\n",
    "        title = search_result['snippet']['title'].lower()\n",
    "\n",
    "        # filters based on brand name and fashion related keywords\n",
    "        if brand_name.lower() in title and any(keyword in title for keyword in keywords):\n",
    "            # filters out social media links\n",
    "            filtered_links = [link for link in links if not any(keyword in link for keyword in social_media_links)]\n",
    "            # get link to video\n",
    "            video_link = f\"https://www.youtube.com/watch?v={video_id}\"\n",
    "            brand_videos.append({\n",
    "                'title': search_result['snippet']['title'],\n",
    "                'links': filtered_links,\n",
    "                'videoLink': video_link\n",
    "            })\n",
    "\n",
    "    # process data and format for csv\n",
    "    brand_youtube_data = []\n",
    "    for video in brand_videos:\n",
    "        if video['links']:\n",
    "            brand_youtube_data.append({\n",
    "                'Title': video['title'],\n",
    "                'Links': '\\n'.join(video['links']),\n",
    "                'VideoLink': video['videoLink']\n",
    "            })\n",
    "\n",
    "    return brand_youtube_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a769906-bb1e-498e-aa3a-2e1542a15124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of brands\n",
    "brands = ['Adidas', 'Nike', 'Sandy Liang']\n",
    "\n",
    "# done\n",
    "# 'Aritzia', 'Skims', 'Shein', 'Princess Polly'\n",
    "# 'Abercrombie & Fitch', 'Abercrombie and Fitch', 'Abercrombie', 'Amazon'\n",
    "# 'Brandy', 'Brandy Melville', 'Uniqlo'\n",
    "# 'Alo Yoga', 'Alo', 'Reformation'\n",
    "# 'Boohoo', 'Nasty Gal', 'Patagonia', 'Hollister'\n",
    "# 'VS', 'Victoria\\'s Secret', 'PINK', 'Victorias Secret'\n",
    "# 'Alice + Olivia', 'Alice & Olivia', 'Alica and Olivia', 'Billabong'\n",
    "# 'Primark', 'Yes Friends', 'ASOS', 'Forever 21'\n",
    "# 'Adidas', 'Nike', 'Sandy Liang'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb578698-1122-48e6-a8de-6e849ecc77af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict to store YouTube data for all brands\n",
    "# keys: brand names, values: data from YouTube\n",
    "all_brand_youtube_data = {}\n",
    "\n",
    "# iterate over each brand and fetch and process data\n",
    "for brand in brands:\n",
    "    youtube_data = get_youtube_data(brand)\n",
    "    # creates key for brand's data and assigns YouTube data to it to add to dict\n",
    "    file_name = f'{brand.lower().replace(\" \", \"_\")}_youtube_data'\n",
    "    all_brand_youtube_data[file_name] = youtube_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "265d2d72-62c7-41f4-95cc-e2fc6f29043b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file saved: ../data/youtube_data/adidas_youtube_data.csv\n",
      "CSV file saved: ../data/youtube_data/nike_youtube_data.csv\n",
      "CSV file saved: ../data/youtube_data/sandy_liang_youtube_data.csv\n"
     ]
    }
   ],
   "source": [
    "# save data to csv files (in youtube_data folder in the data folder)\n",
    "for name, data in all_brand_youtube_data.items():\n",
    "    filename = f\"../data/youtube_data/{name}.csv\"\n",
    "    pd.DataFrame(data).to_csv(filename, index=False)\n",
    "    print(f\"CSV file saved: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbae562e-27b0-4fa7-ad8b-c2a388f2b3ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
