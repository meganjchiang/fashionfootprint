








from googleapiclient.discovery import build
import pandas as pd
from datetime import datetime
import re

# set up YouTube Data API 
api_key = "AIzaSyB4rEWrBMhi4lJEgfwsV386f44qwL3HxG4"
youtube = build('youtube', 'v3', developerKey=api_key)


# search by keywords
keywords = ['haul', 'clothing', 'clothes', 'shop', 'shopping', 'try on', 'try-on']

# set time period to be in 2023
published_after = datetime(2023, 1, 1).isoformat() + 'Z'
published_before = datetime(2023, 12, 31).isoformat() + 'Z'


# search for videos
search_response = youtube.search().list(
    q=keywords, 
    part='snippet',
    type='video',
    publishedAfter=published_after,
    publishedBefore=published_before,
    maxResults=1 # returns 1 result(video) that match w/criteria
).execute()


# process results
videos = []
for search_result in search_response.get('items', []):
    # get and store video id
    video_id = search_result['id']['videoId']
    video_response = youtube.videos().list(
        # receive snippet part of data - title, description, tags, etc.
        part="snippet",
        id=video_id
    ).execute()

    # accesses description field of snipper
    description = video_response['items'][0]['snippet']['description']
    
    # extract links from description
    links = re.findall(r'(https?://\S+)', description)
    
    # add video title, description, and links (if there are links) to videos list
    videos.append({
        'title': search_result['snippet']['title'],
        'links': links
    })


# filter vids into separate lists based on keywords
filtered_vids_keywords = {}

for keyword in keywords:
    filtered_vids_keywords[keyword] = [
        video for video in videos 
        if keyword.lower() in video['title'].lower() and video['links']
    ]
for keyword, vid_list in filtered_vids_keywords.items():
    print(f"VIDEOS WITH '{keyword}':")
    for video in vid_list:
        print("title:", video['title'])
        print("links:")
        for link in video['links']:
            print(link)
        print()











social_media_links = ['pinterest', 'youtube', 'twitter', 'instagram', 'tiktok', 'reddit', 'twitch', 'facebook', 'thmatc']


# search for videos
am_eagle_search_results = youtube.search().list(
    q='American Eagle', # search by a specific brand rather than set of fashion-related keywords
    part='snippet',
    type='video',
    publishedAfter=published_after,
    publishedBefore=published_before,
    maxResults=50
).execute()


am_eagle_videos = []
for search_result in am_eagle_search_results.get('items', []):
    video_id = search_result['id']['videoId']
    video_response = youtube.videos().list(
        part="snippet",
        id=video_id
    ).execute()

    description = video_response['items'][0]['snippet']['description']
    
    links = re.findall(r'(https?://\S+)', description)

    # makes all titles lowercase so code can match on any version of title:
    # (e.g., American Eagle, american eagle, AMERICAN EAGLE)
    title = search_result['snippet']['title'].lower()

    # filters based on 'american eagle' in title and fashion-related keywords
    if 'american eagle' in title and any(keyword in title for keyword in keywords):
        # filters out social media links
        filtered_links = [link for link in links if not any(keyword in link for keyword in social_media_links)]

        am_eagle_videos.append({
            'title': search_result['snippet']['title'],
            'links': filtered_links
        })


# only output videos with links in bios we can scrape
am_eagle_youtube_data = []

for video in am_eagle_videos:
    # check if vid has links
    if video['links']:
        # append the video to the filtered list
        am_eagle_youtube_data.append({
            'Title': video['title'],
            'Links': '\n'.join(video['links'])
        })

# formatted this way so it can be easily converted to csv using to_csv function from pandas
print(am_eagle_youtube_data)














from bs4 import BeautifulSoup
import requests
import time





url = "https://www.ae.com/us/en/p/women/t-shirts/view-all-t-shirts-/ae-hey-baby-ribbed-t-shirt/2370_9630_275?menu=cat4840004"






page = requests.get(url)
soup = BeautifulSoup(page.text, "html.parser")
composition_elements = soup.find_all(string=lambda text: '% ' in str(text).lower())
if composition_elements:
    for element in composition_elements:
        print(element)
else:
    print("none")

















from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from lxml import html





url = "https://www.ae.com/us/en/p/women/t-shirts/view-all-t-shirts-/ae-hey-baby-ribbed-t-shirt/2370_9630_275?menu=cat4840004"
interactive_element_xpath = '//*[@id="main-content-focus"]/div[2]/div[2]/div[2]/div/div[3]/div/div[1]/div[1]'
loaded_content_xpath = '//*[@id="main-content-focus"]/div[2]/div[2]/div[2]/div/div[3]/div/div[1]/div[2]/div/div[2]'





def scrape_american_eagle(url, interactive_element_xpath, loaded_content_xpath, wait_time=10):
    try:
        # Set Chrome 
        chrome_options = Options()

        # Initialize the WebDriver 
        driver = webdriver.Chrome(options=chrome_options)
        
        # Open the webpage
        driver.get(url)

        # Wait for the specified time before clicking the interactive element
        time.sleep(wait_time)  # Wait for the specified time in seconds

        # Find the interactive element
        interactive_element = driver.find_element(By.XPATH, interactive_element_xpath)
        
        # Click the interactive element
        interactive_element.click()

        # Wait for the loaded content to be visible
        loaded_element = WebDriverWait(driver, wait_time).until(
            EC.visibility_of_element_located((By.XPATH, loaded_content_xpath))
        )

        # Once loaded, scrape the content
        dynamic_content = loaded_element.text
        
        return dynamic_content
        
    except Exception as e:
        print(f"An error occurred: {str(e)}")
        return None
        
    finally:
        # Close the WebDriver
        driver.quit()


dynamic_content = scrape_american_eagle(url, interactive_element_xpath, loaded_content_xpath)
if dynamic_content:
    print(dynamic_content)














# step 1: create a list of brands

brands = [
    'Princess Polly',
    'Brandy Melville',
    'Shein',
    'Nike',
    'Abercrombie & Fitch',
    'Amazon',
    'ASOS',
    'Forever 21', 
    'American Eagle',
    'Alo',
    'Reformation',
    'Acne Studios',
    'Alice + Olivia',
    'Sandy Liang',
    'Billabong',
    'Adidas',
    'Aritzia',
    'Uniqlo',
    'Area',
    'Balenciaga',
    'Bottega Veneta',
    'Brooks Brothers',
    'Burberry',
    'Chanel',
    'Coach',
    'Fendi',
    'Gucci',
    'Hermes',
    'Louis Vuitton',
    'Prada',
    'Ralph Lauren',
    'Saint Laurent',
    'Stella McCartney',
    'Telfar',
    'The Row',
    'Theory',
    'Tom Ford',
    'Tory Burch',
    'Valentino',
    '7 For All Mankind',
    "Arc'teryx",
    'aventura',
    'Banana Republic',
    'Boden',
    'Buck Mason',
    'Calvin Klein',
    'Carhartt',
    'Christy Dawn',
    'Columbia',
    'Cotopaxi',
    'Dickies',
    'Djerf Avenue',
    'Doen',
    'Edikted',
    'Everlane',
    'Faithfull the Brand',
    'Frankies Bikinis',
    'Girlfriend Collective',
    'Good American',
    'House of Sunny',
    'J.Crew',
    "Levi's",
    'Madewell',
    'Organic Basics',
    'Pact',
    'Patagonia',
    'prAna',
    'Quince',
    'RE/DONE',
    'Réalisation Par',
    'REI',
    'Sezane',
    'Spanx',
    'Summersalt',
    'tentree',
    'The North Face',
    'Tommy Hilfiger',
    'True Religion',
    'Wrangler',
    'Yes Friends',
    'Aeropostale',
    'Boohoo',
    'Cider',
    'Fashion Nova',
    'GUESS',
    'Hollister',
    'Hot Topic',
    'House of CB',
    'Mango',
    'Missguided',
    'Nasty Gal',
    'PacSun',
    'PrettyLittleThing',
    'Primark',
    'Romwe',
    'Temu',
    'Topshop',
    'Torrid',
    'Under Armour',
    "Victoria's Secret",
    'Yesstyle'
]

# brands added manually (only overall rating is provided): 
# - Ann Taylor
# - Aerie
# - Garage
# - Pink


# step 2: get data (ratings and description) from each brand

import requests
from bs4 import BeautifulSoup
import pandas as pd

all_brand_info = []
cols = ['brand', 'overall_rating', 'planet_score', 'people_score', 'animals_score', 'description']

url = 'https://directory.goodonyou.eco/brand/'

for brand in brands:
    try:
        # convert brand name to url format
        brand_converted = brand.lower() # lowercase
        brand_converted = brand_converted.replace(' ', '-') # replace spaces with dashes
        brand_converted = brand_converted.replace('&', 'and') # replace '&' with 'and'
        brand_converted = brand_converted.replace('+', '-') # replace '+' with '-'
        brand_converted = brand_converted.replace("'", '') # remove apostrophes
        brand_converted = brand_converted.replace('.', '') # remove periods
        brand_converted = brand_converted.replace('/', '') # remove slashes
        brand_converted = brand_converted.replace('é', 'e') # remove accent

        # get data on each brand's page
        response = requests.get(url+brand_converted)
        soup = BeautifulSoup(response.text, 'html.parser')

        # note: when finding classes on GOU's website, you might need to disable JavaScript (since BeautifulSoup can't load dynamic content)

        # get overall rating 
        overall_rating = soup.find('h6', class_='StyledHeading-sc-1rdh4aw-0 jNSEQB id__OverallRating-sc-12z6g46-7 cjSjNJ')
        overall_rating = overall_rating.text.split(': ')[1]

        # get subratings for planet, people, and animals
        subratings = soup.find_all('div', class_='id__RatingSingle-sc-12z6g46-9 ksJKxw')
    
        # remove category name from text 
        subratings[0] = subratings[0].text.split('Planet')[1]
        subratings[1] = subratings[1].text.split('People')[1]
        subratings[2] = subratings[2].text.split('Animals')[1]

        # if there's a rating, convert to int (makes it easier to analyze later)
        for i, rating in enumerate(subratings):
            if rating != 'Not applicable':
                subratings[i] = int(rating.split(' ')[0])

        # get description/justification
        text = soup.find('div', class_='id__BodyText-sc-12z6g46-15 eUqrmK').text
        
        # create new list of current brand info and add data
        brand_info = []
        brand_info.append(brand)
        brand_info.append(overall_rating)
        brand_info.append(subratings[0])
        brand_info.append(subratings[1])
        brand_info.append(subratings[2])
        brand_info.append(text)
        
        # add to overall list of brand info
        all_brand_info.append(brand_info)
    except:
        print(f"{brand} is not in Good On You's Drectory")


# step 3: add certain brands (with limited data provided) manually 

# aerie
aerie_brand_info = []
aerie_brand_info.append('Aerie')
aerie_brand_info.append(2)
aerie_brand_info.append('Not applicable')
aerie_brand_info.append('Not applicable')
aerie_brand_info.append('')

# ann taylor
ann_taylor_brand_info = []
ann_taylor_brand_info.append('Ann Taylor')
ann_taylor_brand_info.append(2)
ann_taylor_brand_info.append('Not applicable')
ann_taylor_brand_info.append('Not applicable')
ann_taylor_brand_info.append('')

# garage
garage_brand_info = []
garage_brand_info.append('Garage')
garage_brand_info.append(2)
garage_brand_info.append('Not applicable')
garage_brand_info.append('Not applicable')
garage_brand_info.append('')

# pink
pink_brand_info = []
pink_brand_info.append('Pink')
pink_brand_info.append(2)
pink_brand_info.append('Not applicable')
pink_brand_info.append('Not applicable')
pink_brand_info.append('')

all_brand_info.append(aerie_brand_info)
all_brand_info.append(ann_taylor_brand_info)
all_brand_info.append(garage_brand_info)
all_brand_info.append(pink_brand_info)


# step 4: create dataframe

brand_df = pd.DataFrame(all_brand_info, columns=cols)

# replace Good On You's categories with numerical ratings
# source: https://saturncloud.io/blog/how-to-convert-categorical-data-to-numerical-data-with-pandas
brand_df['overall_rating'] = brand_df['overall_rating'].replace({
    'We avoid': 1,
    'Not good enough': 2,
    "It's a start": 3,
    'Good': 4,
    'Great': 5
})


brand_df


## step 5: export to a CSV file
# this CSV file is saved in the 'data' folder
brand_df.to_csv('../data/brand_info.csv')











## step 1: get brands recommended by Good On You

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import time

categories = [
    'tops',
    'dresses',
    'basics',
    'bottoms',
    'denim',
    'outerwear',
    'knitwear',
    'activewear',
    'sleepwear'
]

brands = set()

# set Chrome options for headless mode
chrome_options = Options()

# initialize WebDriver with headless mode
driver = webdriver.Chrome(options=chrome_options)

for category in categories:
    # open webpage
    driver.get('https://directory.goodonyou.eco/categories/' + category)

    # get initial height of the page
    last_height = driver.execute_script("return document.body.scrollHeight")

    while True:
        # scroll
        # source: https://stackoverflow.com/questions/73792388/how-to-scroll-to-the-bottom-of-the-page-with-selenium-python
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        
        # wait for 2 seconds
        time.sleep(2)
        
        # get new height of page
        new_height = driver.execute_script("return document.body.scrollHeight")
        
        # stop once it gets to the bottom
        if new_height == last_height:
            break
        
        # update height
        last_height = new_height

    # 100 results are now shown on the page
    for i in range(1, 101):
        # get xpath for each brand
        brand_xpath = f'//*[@id="__next"]/div/div[4]/div/div[2]/div/div/div[{i}]/div/div/div[2]/h5/a'

        # get brand element (after 5 second delay)
        brand_element = WebDriverWait(driver, 5).until(
            EC.visibility_of_element_located((By.XPATH, brand_xpath))
        )

        # get brand name and link
        # source for link: https://stackoverflow.com/questions/54862426/python-selenium-get-href-value
        brand_name = brand_element.text
        brand_link = brand_element.get_attribute('href')

        brands.add((brand_name, brand_link))
     
# close the WebDriver
driver.quit()


## step 2: get data (ratings and description) from each brand

# create list of all recommended brands + their info
recommended = []
cols = ['brand', 'overall_rating', 'planet_score', 'people_score', 'animals_score', 'description']

for brand in brands:
    brand_name = brand[0]
    brand_href = brand[1]

    # get data on each brand's page
    response = requests.get(brand_href)
    soup = BeautifulSoup(response.text, 'html.parser')

    # get overall rating 
    overall_rating = soup.find('h6', class_='StyledHeading-sc-1rdh4aw-0 jNSEQB id__OverallRating-sc-12z6g46-7 cjSjNJ')
    overall_rating = overall_rating.text.split(': ')[1]
    
    # get subratings for planet, people, and animals
    subratings = soup.find_all('div', class_='id__RatingSingle-sc-12z6g46-9 ksJKxw')
    
    # remove category name from text 
    subratings[0] = subratings[0].text.split('Planet')[1]
    subratings[1] = subratings[1].text.split('People')[1]
    subratings[2] = subratings[2].text.split('Animals')[1]

    # if there's a rating, convert to int (makes it easier to analyze later)
    for i, rating in enumerate(subratings):
        if rating != 'Not applicable':
            subratings[i] = int(rating.split(' ')[0])

    # get description/justification
    text = soup.find('div', class_='id__BodyText-sc-12z6g46-15 eUqrmK').text

    # create new list of current brand info and add data
    brand_info = []
    brand_info.append(brand_name)
    brand_info.append(overall_rating)
    brand_info.append(subratings[0])
    brand_info.append(subratings[1])
    brand_info.append(subratings[2])
    brand_info.append(text)
    
    # add to overall list of brand info
    recommended.append(brand_info)


## step 3: create dataframe

recommended_df = pd.DataFrame(recommended, columns=cols)

# replace Good On You's categories with numerical ratings
# source: https://saturncloud.io/blog/how-to-convert-categorical-data-to-numerical-data-with-pandas
recommended_df['overall_rating'] = recommended_df['overall_rating'].replace({
    'We avoid': 1,
    'Not good enough': 2,
    "It's a start": 3,
    'Good': 4,
    'Great': 5
})


recommended_df


## step 4: export to a CSV file

# this CSV file is saved in the 'data' folder
recommended_df.to_csv('../data/gou_recommended.csv')











# imports
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from bs4 import BeautifulSoup
import pandas as pd
import re
import csv


# create session
session = requests.Session()
# retry three times in case of exception
retry = Retry(connect=3, backoff_factor=0.5)
# apply delays between attempts
adapter = HTTPAdapter(max_retries=retry)
session.mount('https://', adapter)


# scrape urls to brand reviews
eco_stylist_brands = session.get("https://www.eco-stylist.com/sustainable-brands/")
content = BeautifulSoup(eco_stylist_brands.text, 'html.parser')
# all links
page_links = content.find_all("a")
# links to brand reviews
brand_urls = set(link.get('href') for link in page_links if ("https://www.eco-stylist.com/ethical-brand/" in link.get('href')) and (link.get('href') != "https://www.eco-stylist.com/ethical-brand/"))

print(brand_urls)


# create dataframe 
eco_stylist_df = pd.DataFrame(columns=['Brand', 'Overall', 'Transparency', 'Fair Labor', 'Sustainably Made', 'URL'])


# scrape for brand data
for url in brand_urls:
    # search for brand review
    review = session.get(url)
    
    try:
        # content
        content = BeautifulSoup(review.text, 'html.parser')

        # brand name
        brand = content.find('h1').get_text()

        # overall rating
        overall = content.find(string=re.compile("Overall Rating:")).split(" ")[2]
        
        # transparency, fair labor, sustainably made
        ratings = content.find_all(string=re.compile("Rated:"))
        transparency = ratings[0].split(" ")[1]
        fair_labor = ratings[1].split(" ")[1]
        sustainably_made = ratings[2].split(" ")[1]

        # update dataframe
        eco_stylist_df.loc[len(eco_stylist_df.index)] = [brand, overall, transparency, fair_labor, sustainably_made, url]

    except:
        print(url + " Failed to Load")


# visualize first 10 rows of the new dataset
eco_stylist_df.head(10)


# export csv file
eco_stylist_df.to_csv('../data/eco_stylist_ratings.csv')








# get page range
first_page_url = "https://sustainablereview.com/brand-ratings/"
first_page = session.get(first_page_url)
content = BeautifulSoup(first_page.text, 'html.parser')
# last page number out of all page numbers
last_page_num = str(content.find_all('a', class_="page-numbers")[-1]).split('>')[1].split('<')[0]

print(last_page_num)


# get brand links from all pages
brand_urls = []
# loop through all pages
for i in range(1, int(last_page_num) + 1):
    # scrape first page
    if i == 1:
        page = first_page
    # scrape remaining pages
    else:
        next_page_url = "https://sustainablereview.com/brand-ratings/?query-48-page="
        page = session.get(next_page_url + str(i))

    # content
    content = BeautifulSoup(page.text, 'html.parser')

    # links to brand reviews
    page_links = content.find_all("a")
    brands = set(link.get('href') for link in page_links if ("https://sustainablereview.com/brand-ratings/" in link.get('href')) and (link.get('href') != "https://sustainablereview.com/brand-ratings/"))

    # update brand urls list
    brand_urls.extend(brands)

print(brand_urls)


# create dataframe
sustainable_review_df = pd.DataFrame(columns=['Brand', 'Rating', 'Factors'])


# scrape for brand data
for url in brand_urls:
    try: 
        # search for brand review
        review = session.get(url)
        
        # content
        content = BeautifulSoup(review.text, 'html.parser')

        # brand
        brand = content.find('h1', class_='post-title').get_text()

        # rating
        information = content.find('div', class_='InfoBox')
        rating = information.find('p').get_text().split(" ")[3]
        
        # factors
        body = content.find('div', class_='col-md-12 col-lg-9')
        factors = str(body.find_all('h3')).split(', ')

        # clean list of factors
        cleaned_factors = []
        for factor in factors: 
            cleaned_factor = factor.replace("[","").replace("]","").replace('<h3>', '').replace('<strong>', '').replace('</h3>', '').replace('</strong>', '').replace("Similar brands:","")
            
            # drop ':' from factor
            if cleaned_factor.endswith(":"):
                cleaned_factor = cleaned_factor[:-1]

            # exclude headings with "Conclusion"
            if "Conclusion" in cleaned_factor:
                cleaned_factor = ""

            # append cleaned factor to list if the item is not emtpy
            if cleaned_factor != "":
                cleaned_factors.append(cleaned_factor)

        # update dataframe
        sustainable_review_df.loc[len(sustainable_review_df.index)] = [brand, rating, ", ".join(cleaned_factors)]
    
    except:
        print(url + " Failed to Load")


# visualize first 10 rows of the new dataset
sustainable_review_df.head(10)


# export csv file
sustainable_review_df.to_csv('../data/sustainable_review_ratings.csv')



