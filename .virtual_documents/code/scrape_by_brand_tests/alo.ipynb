import pandas as pd
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from lxml import html
from bs4 import BeautifulSoup
import requests
import time


def scrape_alo(url):
    try:
        # Set Chrome options for headless mode
        chrome_options = Options()
        chrome_options.add_argument("--headless")
        # Initialize the WebDriver
        driver = webdriver.Chrome(options=chrome_options)
        driver.get(url)
        
        # Wait for the data element to be present on the page
        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, "fabrication")))
        
        # Get the page source after interactions
        page_source = driver.page_source
        
        # Close the WebDriver once done to avoid multiple instances
        driver.quit()
        
        # Parse the page source with Beautiful Soup
        soup = BeautifulSoup(page_source, "html.parser")
        
        # Extract data using Beautiful Soup methods
        data_title = soup.find("h1", class_="productTitle")
        data_element = soup.find("div", class_="fabrication")
        
        if data_element:
            return data_title.text, data_element.text
        else:
            return "Data element not found on the page."
    except Exception as e:
        return f"An error occurred: {str(e)}"



df = pd.read_csv("../../data/youtube_data/alo_youtube_data.csv")
top_row = df.iloc[4]
links_list = top_row["Links"].split('\n')

for link in links_list:
    print(f"Scraping data from link: {link}")
    scraped_data = scrape_alo(link)
    print(scraped_data + "\n")
    time.sleep(3)


url = "https://rstyle.me/cz-n/hgjsvqd3q47"

scrape_alo(url)
