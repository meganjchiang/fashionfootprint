import pandas as pd
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from lxml import html
from bs4 import BeautifulSoup
import requests
import time


## version 2
def scrape_adidas(url, wait_time=20):
    try:
        user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.71 Safari/537.36"
        interactive_element_xpath = '//*[@id="navigation-target-specifications"]/div/button'
        loaded_content_xpath = '//*[@id="navigation-target-specifications"]/div/div/div/div'

        # Set Chrome options for headless mode
        chrome_options = Options()
        chrome_options.add_argument(f"user-agent={user_agent}")
        chrome_options.add_argument("--headless")

        # Initialize the WebDriver with headless mode
        driver = webdriver.Chrome(options=chrome_options)
        
        # Open the webpage
        driver.get(url)

        # Wait for the interactive element to be visible
        WebDriverWait(driver, wait_time).until(
            EC.visibility_of_element_located((By.XPATH, interactive_element_xpath))
        )

        # Find the interactive element
        interactive_element = driver.find_element(By.XPATH, interactive_element_xpath)
        
        # Click the interactive element
        interactive_element.click()

        # Wait for the loaded content to be visible
        loaded_element = WebDriverWait(driver, wait_time).until(
            EC.visibility_of_element_located((By.XPATH, loaded_content_xpath))
        )

        # Once loaded, scrape the content
        dynamic_content = loaded_element.text
        
        return dynamic_content
        
    except Exception as e:
        print("An error occurred")
        return None
        
    finally:
        # Close the WebDriver
        driver.quit()


df = pd.read_csv("../../data/youtube_data/adidas_youtube_data.csv")
top_row = df.iloc[1]
links_list = top_row["Links"].split('\n')

for link in links_list:
    print(f"Scraping data from Adidas link: {link}")
    scraped_data = scrape_adidas(link)
    if scraped_data:
        print(scraped_data + "\n")
    else:
        print("Failed to scrape data from the link.\n")
    time.sleep(3)
