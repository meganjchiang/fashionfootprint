import pandas as pd


from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from lxml import html
from bs4 import BeautifulSoup
import requests
import time


def scrape_abercrombie(url):
    try:
        # Set Chrome options for headless mode
        chrome_options = Options()
        chrome_options.add_argument("--headless")
        
        # Initialize the WebDriver
        with webdriver.Chrome(options=chrome_options) as driver:
            driver.get(url)
            
            # Wait for the page to fully load
            WebDriverWait(driver, 25).until(EC.presence_of_element_located((By.CSS_SELECTOR, 'body')))
            
            # Get the page source after interactions
            page_source = driver.page_source
        
        # Parse the page source with Beautiful Soup
        soup = BeautifulSoup(page_source, "html.parser")
        
        # Find the <div> tag with class "product__description rte"
        data_element = soup.find("h4", class_="h4 fabric-care-mfe__label")
        
        if data_element:
            return data_element.text.strip()
        else:
            return "Link no longer available."
            
    except Exception as e:
        return f"An error occurred: {str(e)}"


abercrombie_videos = pd.read_csv("../../data/youtube_data/abercombie_youtube_data.csv")



def extract_links(links_str):
    return links_str.split('\n')


df = pd.read_csv("../../data/youtube_data/abercombie_youtube_data.csv")
top_row = df.iloc[0]
links_list = top_row["Links"].split('\n')

for link in links_list:
    print(f"Scraping data from link: {link}")
    scraped_data = scrape_abercrombie(link)
    print(scraped_data + "\n")
    time.sleep(3)
