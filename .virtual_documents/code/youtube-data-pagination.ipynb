








from googleapiclient.discovery import build
import pandas as pd
from datetime import datetime
import re
import os


# set up YouTube Data API 
api_key = "AIzaSyB4rEWrBMhi4lJEgfwsV386f44qwL3HxG4"
youtube = build('youtube', 'v3', developerKey=api_key)





def get_youtube_data(brand_name):
    published_after = datetime(2023, 1, 1).isoformat() + 'Z'
    published_before = datetime(2023, 12, 31).isoformat() + 'Z'
    keywords = ['haul', 'clothing', 'clothes', 'shop', 'shopping', 'try on', 'try-on', 'review', 'styling']
    social_media_links = ['pinterest', 'youtube', 'twitter', 'instagram', 'tiktok',
                          'reddit', 'twitch', 'facebook', 'thmatc', 'spotify']

    # fetch initial search results
    search_results = []
    # nextPageToken is returned when there are more results available,
    # but those results are spread across multiple pages
    # Fetch the initial search results
    # next_page_token = None
    # response = None 
    # while True:
    #     # send request to the YouTube  API
    #     request = youtube.search().list(
    #         q=brand_name,
    #         part='snippet',
    #         type='video',
    #         publishedAfter=published_after,
    #         publishedBefore=published_before,
    #         maxResults=50,
    #         pageToken=next_page_token
    #     )
    #     response = request.execute()
    #     # add search result list w/items from response
    #     search_results.extend(response.get('items', []))
    #     # get next page token for pagination
    #     next_page_token = response.get('nextPageToken')
    #     # check if there are no more pages left
    #     if not next_page_token:
    #         break
    # initialize request, response, next_page_token
    request = youtube.search().list(
        q=brand_name,
        part='snippet',
        type='video',
        publishedAfter=published_after,
        publishedBefore=published_before,
        maxResults=50
    )
    response = request.execute()
    next_page_token = response.get('nextPageToken')
    
    while next_page_token is not None:
        # send request to YouTube API
        request = youtube.search().list(
            q=brand_name,
            part='snippet',
            type='video',
            publishedAfter=published_after,
            publishedBefore=published_before,
            maxResults=50,
            pageToken=next_page_token
        )
        response = request.execute()
        # add items from response 
        search_results.extend(response.get('items',[]))
        # get next page token for pagination
        next_page_token = response.get('nextPageToken')

    # process search results to extract relevant vid data
    brand_videos = []
    for search_result in search_results:
        # gets and stores video id
        video_id = search_result['id']['videoId']
        video_response = youtube.videos().list(
            # receive snippet part of data - title, description, tags, etc.
            part="snippet",
            id=video_id
        ).execute()

        # access description field of snipper
        description = video_response['items'][0]['snippet']['description']
        # extract links from description
        links = re.findall(r'(https?://\S+)', description)
        # makes all titles lowercase so code can match on any version of title:
        title = search_result['snippet']['title'].lower()

        # filters based on brand name and fashion related keywords
        if brand_name.lower() in title and any(keyword in title for keyword in keywords):
            # filters out social media links
            filtered_links = [link for link in links if not any(keyword in link for keyword in social_media_links)]
            # get link to video
            video_link = f"https://www.youtube.com/watch?v={video_id}"
            brand_videos.append({
                'title': search_result['snippet']['title'],
                'links': filtered_links,
                'videoLink': video_link
            })

    # process data and format for csv
    brand_youtube_data = []
    for video in brand_videos:
        if video['links']:
            brand_youtube_data.append({
                'Title': video['title'],
                'Links': '\n'.join(video['links']),
                'VideoLink': video['videoLink']
            })

    return brand_youtube_data


# list of brands
brands = ['Uniqlo', 'Brandy', 'Brandy Melville']


# dict to store YouTube data for all brands
# keys: brand names, values: data from YouTube
all_brand_youtube_data = {}

# iterate over each brand and fetch and process data
for brand in brands:
    youtube_data = get_youtube_data(brand)
    # creates key for brand's data and assigns YouTube data to it to add to dict
    file_name = f'{brand.lower().replace(" ", "_")}_youtube_data'
    all_brand_youtube_data[file_name] = youtube_data


# save data to csv files (in youtube_data folder in the data folder)
for name, data in all_brand_youtube_data.items():
    filename = f"../data/youtube_data/{name}.csv"
    pd.DataFrame(data).to_csv(filename, index=False)
    print(f"CSV file saved: {filename}")



