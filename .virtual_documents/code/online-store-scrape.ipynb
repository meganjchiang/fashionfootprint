from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from lxml import html
from bs4 import BeautifulSoup
import requests
import time


def scrape_princess_polly(url):
    try:
        # Set Chrome options for headless mode
        chrome_options = Options()
        chrome_options.add_argument("--headless")
        # Initialize the WebDriver
        driver = webdriver.Chrome(options=chrome_options)
        driver.get(url)
        # Get the page source after interactions
        page_source = driver.page_source
        # Close the WebDriver once done to avoid multiple instances
        driver.quit()
        # Parse the page source with Beautiful Soup
        soup = BeautifulSoup(page_source, "html.parser")
        # Extract data using Beautiful Soup methods
        data_element = soup.find("div", class_="product-details__content")
        if data_element:
            return data_element.text
        else:
            return "Data element not found on the page."
    except Exception as e:
        return f"An error occurred: {str(e)}"

# URL of the webpage you want to scrape
url = "https://us.princesspolly.com/products/caruso-denim-wrap-skort-light-wash"
# Call the function to scrape the website
data = scrape_princess_polly(url)
print(data)


def scrape_shein(url):
    try:
        # Set Chrome options for headless mode
        chrome_options = Options()
        chrome_options.add_argument("--headless")
        # Initialize the WebDriver
        driver = webdriver.Chrome(options=chrome_options)
        driver.get(url)
        # Get the page source after interactions
        page_source = driver.page_source
        # Close the WebDriver once done to avoid multiple instances
        driver.quit()
        # Parse the page source with Beautiful Soup
        soup = BeautifulSoup(page_source, "html.parser")
        # Find the div with class "key" containing "Composition: "
        composition_key = soup.find("div", class_="key", string="Composition: ")
        if composition_key:
            # Get the next sibling of the "key" div, which contains the composition information
            composition_val = composition_key.find_next_sibling("div", class_="val")
            if composition_val:
                # Extract the composition information
                composition_text = composition_val.text.strip()
                return composition_text
            else:
                return "Composition information not found."
        else:
            return "Composition key not found."
    except Exception as e:
        return f"An error occurred: {str(e)}"

# URL of the webpage you want to scrape
url = "https://us.shein.com/Solid-Bandeau-Bra-p-12216010.html?src_module=All&src_identifier=on=PRODUCT_ITEMS_COMPONENT`cn=salezone`hz=0`ps=4_1_0`jc=itemPicking_100546960&src_tab_page_id=page_home1713242748965&mallCode=1&imgRatio=3-4"
# Call the function to scrape the website
data = scrape_shein(url)
print(data)


def scrape_nike(url):
    try:
        # Set Chrome options for headless mode
        chrome_options = Options()
        chrome_options.add_argument("--headless")
        # Initialize the WebDriver
        driver = webdriver.Chrome(options=chrome_options)
        driver.get(url)
        # Get the page source after interactions
        page_source = driver.page_source
        # Close the WebDriver once done to avoid multiple instances
        driver.quit()
        # Parse the page source with Beautiful Soup
        soup = BeautifulSoup(page_source, "html.parser")
        # Extract data using Beautiful Soup methods
        data_element = soup.find("div", class_="pi-pdpmainbody")
        if data_element:
            return data_element.text
        else:
            return "Data element not found on the page."
    except Exception as e:
        return f"An error occurred: {str(e)}"
    
# URL of the webpage you want to scrape
url = "https://www.nike.com/t/jordan-paris-saint-germain-pullover-hoodie-big-kids-hoodie-tpqdWh/45C566-023?nikemt=true&cp=75009274456_search_&gad_source=1&gclid=CjwKCAjwoPOwBhAeEiwAJuXRh47Ul92JN0-vlgv6RcIG0mKBklxgk4gs4zOPV8cguxRM7hK9eK_DThoCLUoQAvD_BwE"
# Call the function to scrape the website
data = scrape_nike(url)
print(data)


def scrape_brandy(url):
    try:
        # Set Chrome options for headless mode
        chrome_options = Options()
        chrome_options.add_argument("--headless")
        
        # Initialize the WebDriver
        with webdriver.Chrome(options=chrome_options) as driver:
            driver.get(url)
            
            # Get the page source after interactions
            page_source = driver.page_source
        
        # Parse the page source with Beautiful Soup
        soup = BeautifulSoup(page_source, "html.parser")
        
        # Find the <div> tag with class "product__description rte"
        data_element = soup.find("div", class_="product__description rte")
        
        if data_element:
            return data_element.text.strip()
        else:
            return "Data element not found on the page."
            
    except Exception as e:
        return f"An error occurred: {str(e)}"

# URL of the webpage you want to scrape
url = "https://us.brandymelville.com/products/zelly-basic-top"
# Call the function to scrape the website
data = scrape_brandy(url)
print(data)


def scrape_abercrombie(url):
    try:
        # Set Chrome options for headless mode
        chrome_options = Options()
        chrome_options.add_argument("--headless")
        
        # Initialize the WebDriver
        with webdriver.Chrome(options=chrome_options) as driver:
            driver.get(url)
            
            # Wait for the page to fully load
            WebDriverWait(driver, 25).until(EC.presence_of_element_located((By.CSS_SELECTOR, 'body')))
            
            # Get the page source after interactions
            page_source = driver.page_source
        
        # Parse the page source with Beautiful Soup
        soup = BeautifulSoup(page_source, "html.parser")
        
        # Find the <div> tag with class "product__description rte"
        data_element = soup.find("h4", class_="h4 fabric-care-mfe__label")
        
        if data_element:
            return data_element.text.strip()
        else:
            return "Link no longer available."
            
    except Exception as e:
        return f"An error occurred: {str(e)}"

# URL of the webpage you want to scrape
url = "https://bit.ly/3UlUXwr"
# Call the function to scrape the website
data = scrape_abercrombie(url)
print(data)




def scrape_amazon(url):
    try:
        # Set Chrome options for headless mode
        chrome_options = Options()
        chrome_options.add_argument("--headless")
        
        # Initialize the WebDriver
        with webdriver.Chrome(options=chrome_options) as driver:
            driver.get(url)
            
            # Get the page source after interactions
            page_source = driver.page_source
        
        # Parse the page source with Beautiful Soup
        soup = BeautifulSoup(page_source, "html.parser")
        
        # Find the <div> tag with class "product__description rte"
        data_element = soup.find("div", class_="a-fixed-left-grid product-facts-detail")
        
        if data_element:
            return data_element.text.strip()
        else:
            return "Data element not found on the page."
            
    except Exception as e:
        return f"An error occurred: {str(e)}"

# URL of the webpage you want to scrape
url = "https://www.amazon.com/Lyss%C3%A9-Womens-Schiffer-Button-Medium/dp/B01EWRKXTE/ref=lp_121173939011_1_2?pf_rd_p=53d84f87-8073-4df1-9740-1bf3fa798149&pf_rd_r=VN92AA0AZ4XSEQDKW4EJ"
# Call the function to scrape the website
data = scrape_amazon(url)
print(data)


def scrape_asos(url, interactive_element_xpath, loaded_content_xpath, wait_time=10):
    try:
        # Set Chrome options for headless mode
        chrome_options = Options()

        # Initialize the WebDriver with headless mode
        driver = webdriver.Chrome(options=chrome_options)
        
        # Open the webpage
        driver.get(url)

        # Wait for the specified time before clicking the interactive element
        time.sleep(wait_time)  # Wait for the specified time in seconds

        # Find the interactive element
        interactive_element = driver.find_element(By.XPATH, interactive_element_xpath)
        
        # Click the interactive element
        interactive_element.click()

        # Wait for the loaded content to be visible
        loaded_element = WebDriverWait(driver, wait_time).until(
            EC.visibility_of_element_located((By.XPATH, loaded_content_xpath))
        )

        # Once loaded, scrape the content
        dynamic_content = loaded_element.text
        
        return dynamic_content
        
    except Exception as e:
        print(f"An error occurred: {str(e)}")
        return None
        
    finally:
        # Close the WebDriver
        driver.quit()

# Example usage:
url = "https://www.asos.com/us/asos-design/asos-design-pleated-mini-skirt-in-gray/prd/205661997#colourWayId-205661998"
interactive_element_xpath = '//*[@id="productDescription"]/ul/li[5]/div/h2'
loaded_content_xpath = '//*[@id="productDescriptionAboutMe"]'

dynamic_content = scrape_asos(url, interactive_element_xpath, loaded_content_xpath)
if dynamic_content:
    print(dynamic_content)



def scrape_f21(url, target_element_xpath, wait_time=10):
    try:
        # Set Chrome options for headless mode
        chrome_options = Options()

        # Initialize the WebDriver with headless mode
        driver = webdriver.Chrome(options=chrome_options)
        
        # Open the webpage
        driver.get(url)

        # Wait for the target element to be visible
        target_element = WebDriverWait(driver, wait_time).until(
            EC.visibility_of_element_located((By.XPATH, target_element_xpath))
        )

        # Once loaded, scrape the content
        dynamic_content = target_element.text
        
        return dynamic_content
        
    except Exception as e:
        print(f"An error occurred: {str(e)}")
        return None
        
    finally:
        # Close the WebDriver
        driver.quit()

# Example usage:
url = "https://www.forever21.com/us/20012622040501.html"
target_element_xpath = '//*[@id="main"]/div[2]/div[1]/div[2]/div[3]/div/div[6]/div/div[1]/section[2]/div'

dynamic_content = scrape_f21(url, target_element_xpath)
if dynamic_content:
    print(dynamic_content)


def scrape_american_eagle(url, interactive_element_xpath, loaded_content_xpath, wait_time=10):
    try:
        # Set Chrome options for headless mode
        chrome_options = Options()

        # Initialize the WebDriver with headless mode
        driver = webdriver.Chrome(options=chrome_options)
        
        # Open the webpage
        driver.get(url)

        # Wait for the specified time before clicking the interactive element
        time.sleep(wait_time)  # Wait for the specified time in seconds

        # Find the interactive element
        interactive_element = driver.find_element(By.XPATH, interactive_element_xpath)
        
        # Click the interactive element
        interactive_element.click()

        # Wait for the loaded content to be visible
        loaded_element = WebDriverWait(driver, wait_time).until(
            EC.visibility_of_element_located((By.XPATH, loaded_content_xpath))
        )

        # Once loaded, scrape the content
        dynamic_content = loaded_element.text
        
        return dynamic_content
        
    except Exception as e:
        print(f"An error occurred: {str(e)}")
        return None
        
    finally:
        # Close the WebDriver
        driver.quit()

# Example usage:
url = "https://www.ae.com/us/en/p/women/high-waisted-jeans/high-waisted-mom-jeans/ae-stretch-mom-jean/0436_4332_110?menu=cat4840004"
interactive_element_xpath = '//*[@id="main-content-focus"]/div[2]/div[2]/div[2]/div/div[3]/div/div[1]/div[1]'
loaded_content_xpath = '//*[@id="main-content-focus"]/div[2]/div[2]/div[2]/div/div[3]/div/div[1]/div[2]/div/div[2]'

dynamic_content = scrape_american_eagle(url, interactive_element_xpath, loaded_content_xpath)
if dynamic_content:
    print(dynamic_content)



def scrape_alo(url):
    try:
        # Set Chrome options for headless mode
        chrome_options = Options()
        chrome_options.add_argument("--headless")
        # Initialize the WebDriver
        driver = webdriver.Chrome(options=chrome_options)
        driver.get(url)
        # Get the page source after interactions
        page_source = driver.page_source
        # Close the WebDriver once done to avoid multiple instances
        driver.quit()
        # Parse the page source with Beautiful Soup
        soup = BeautifulSoup(page_source, "html.parser")
        # Extract data using Beautiful Soup methods
        data_element = soup.find("div", class_="fabrication")
        if data_element:
            return data_element.text
        else:
            return "Data element not found on the page."
    except Exception as e:
        return f"An error occurred: {str(e)}"

# URL of the webpage you want to scrape
url = "https://www.aloyoga.com/products/w9538r-airbrush-stream-lined-bra-tank-ivory-black?variant=41346126512308&disableCurrencyEstimate&gad_source=1&gclid=CjwKCAjww_iwBhApEiwAuG6ccL77JcerRoX7_JAHcpKdcfgONVm-BMBBTVNxENk7-9cIIDnlReDxGxoCaegQAvD_BwE"
# Call the function to scrape the website
data = scrape_alo(url)
print(data)


def scrape_reformation(url):
    try:
        page = requests.get(url)
        soup = BeautifulSoup(page.text, "html.parser")
        composition_elements = soup.find_all(string=lambda text: '% ' in str(text).lower())
        if composition_elements:
            for element in composition_elements:
                print(element)
        else:
            print("none")
    except Exception as e:
        return f"An error occured {str(e)}"
    
url = "https://www.thereformation.com/products/vida-low-rise-pant/1314838BLK.html?dwvar_1314838BLK_color=BLK&_gl=1*13y89uh*_up*MQ..&gclid=CjwKCAjww_iwBhApEiwAuG6ccHuReQ50wHf1NGMlZG5XTJoqFuugi5BXX5mlzOcjB35u29kXoUvEwBoCbf4QAvD_BwE&atc_confirmation=true"
data = scrape_reformation(url)
print(data)


def scrape_acne_studios(url):
    try:
        # Set Chrome options for headless mode
        chrome_options = Options()
        chrome_options.add_argument("--headless")
        # Initialize the WebDriver
        driver = webdriver.Chrome(options=chrome_options)
        driver.get(url)
        # Get the page source after interactions
        page_source = driver.page_source
        # Close the WebDriver once done to avoid multiple instances
        driver.quit()
        # Parse the page source with Beautiful Soup
        soup = BeautifulSoup(page_source, "html.parser")
        # Extract data using Beautiful Soup methods
        data_element = soup.find("div", id="productDescription")
        if data_element:
            return data_element.text
        else:
            return "Data element not found on the page."
    except Exception as e:
        return f"An error occurred: {str(e)}"

# URL of the webpage you want to scrape
url = "https://www.acnestudios.com/us/en/wool-mohair-scarf---narrow-lavender-purple/CA0290-ADH.html?g=woman"
# Call the function to scrape the website
data = scrape_acne_studios(url)
print(data)


def scrape_alice_and_olivia(url):
    try:
        # Set Chrome options for headless mode
        chrome_options = Options()
        chrome_options.add_argument("--headless")
        # Initialize the WebDriver
        driver = webdriver.Chrome(options=chrome_options)
        driver.get(url)
        # Get the page source after interactions
        page_source = driver.page_source
        # Close the WebDriver once done to avoid multiple instances
        driver.quit()
        # Parse the page source with Beautiful Soup
        soup = BeautifulSoup(page_source, "html.parser")
        # Extract data using Beautiful Soup methods
        data_element = soup.find("div", class_="details")
        if data_element:
            return data_element.text
        else:
            return "Data element not found on the page."
    except Exception as e:
        return f"An error occurred: {str(e)}"

# URL of the webpage you want to scrape
url = "https://www.aliceandolivia.com/rosalee-tie-strap-bustier-maxi-dress/CC404P22523G123.html?lang=default"
# Call the function to scrape the website
data = scrape_alice_and_olivia(url)
print(data)


def scrape_sandy_liang(url):
    try:
        # Set Chrome options for headless mode
        chrome_options = Options()
        chrome_options.add_argument("--headless")
        
        # Initialize the WebDriver
        driver = webdriver.Chrome(options=chrome_options)
        
        # Open the webpage
        driver.get(url)
        
        # Wait for the page to fully load
        time.sleep(5)  # Adjust the wait time as needed
        
        # Get the page source after interactions
        page_source = driver.page_source
        
        # Close the WebDriver once done to avoid multiple instances
        driver.quit()
        
        # Parse the page source with lxml and XPath
        tree = html.fromstring(page_source)
        
        # Extract data using XPath expressions
        data_element = tree.xpath('//*[@id="product-collapse-description"]/p[3]/span/span/text()')
        
        if data_element:
            return ''.join(data_element).strip()
        else:
            return "Data element not found on the page."
            
    except Exception as e:
        return f"An error occurred: {str(e)}"

# URL of the webpage you want to scrape
url = "https://www.sandyliang.info/products/primo-dress"
# Call the function to scrape the website
data = scrape_sandy_liang(url)
print(data)



def scrape_billabong(url):
    try:
        # Set Chrome options for headless mode
        chrome_options = ChromeOptions()
        chrome_options.add_argument("--headless")
        
        # Initialize the WebDriver
        driver = Chrome(options=chrome_options)
        
        # Open the webpage
        driver.get(url)
        
        # Wait for the page to fully load
        time.sleep(5)  # Adjust the wait time as needed
        
        # Get the page source after interactions
        page_source = driver.page_source
        
        # Close the WebDriver once done to avoid multiple instances
        driver.quit()
        
        # Parse the page source with lxml and XPath
        tree = html.fromstring(page_source)
        
        # Extract data using XPath expressions
        data_element = tree.xpath('//*[@id="shopify-section-template--22415043658042__product-essentials"]/section/article[2]/div[8]/details[2]/div/div/div[1]/p[4]/text()')
        
        if data_element:
            return ''.join(data_element).strip()
        else:
            return "Data element not found on the page."
            
    except Exception as e:
        return f"An error occurred: {str(e)}"

# URL of the webpage you want to scrape
url = "https://www.billabong.com/collections/womens-clothing-tops/products/faye-knit-1"
# Call the function to scrape the website
data = scrape_billabong(url)
print(data)



def scrape_billabong(url):
    try:
        # Set Chrome options for headless mode
        chrome_options = ChromeOptions()
        chrome_options.add_argument("--headless")
        
        # Initialize the WebDriver
        driver = Chrome(options=chrome_options)
        
        # Open the webpage
        driver.get(url)
        
        # Wait for the page to fully load
        time.sleep(5)  # Adjust the wait time as needed
        
        # Get the page source after interactions
        page_source = driver.page_source
        
        # Close the WebDriver once done to avoid multiple instances
        driver.quit()
        
        # Parse the page source with lxml and XPath
        tree = html.fromstring(page_source)
        
        # Extract data using XPath expressions
        data_element = tree.xpath('//*[@id="shopify-section-template--22415043658042__product-essentials"]/section/article[2]/div[8]/details[2]/div/div/div[1]/p[4]/text()')
        
        if data_element:
            return ''.join(data_element).strip()
        else:
            return "Data element not found on the page."
            
    except Exception as e:
        return f"An error occurred: {str(e)}"

# URL of the webpage you want to scrape
url = "https://www.billabong.com/collections/womens-clothing-tops/products/faye-knit-1"
# Call the function to scrape the website
data = scrape_billabong(url)
print(data)



def scrape_boohoo(url):
    try:
        # Set Chrome options for headless mode
        chrome_options = Options()
        chrome_options.add_argument("--headless")
        # Initialize the WebDriver
        driver = webdriver.Chrome(options=chrome_options)
        driver.get(url)
        # Get the page source after interactions
        page_source = driver.page_source
        # Close the WebDriver once done to avoid multiple instances
        driver.quit()
        # Parse the page source with Beautiful Soup
        soup = BeautifulSoup(page_source, "html.parser")
        # Extract data using Beautiful Soup methods
        data_element = soup.find("p", class_="b-product_details-composition")
        if data_element:
            return data_element.text
        else:
            return "Data element not found on the page."
    except Exception as e:
        return f"An error occurred: {str(e)}"

# URL of the webpage you want to scrape
url = "https://us.boohoo.com/lapel-crop-blazer/DZZ52779.html?color=123&_gl=1*m20h85*_up*MQ..*_ga*MjAyMzc4NTAyNi4xNzEzMzk2ODY5*_ga_CKX55DLD7G*MTcxMzM5Njg2OC4xLjEuMTcxMzM5Njk1My4wLjAuMA.."
# Call the function to scrape the website
data = scrape_boohoo(url)
print(data)


def scrape_nasty_gal(url):
    try:
        # Set Chrome options for headless mode
        chrome_options = Options()
        chrome_options.add_argument("--headless")
        # Initialize the WebDriver
        driver = webdriver.Chrome(options=chrome_options)
        driver.get(url)
        # Get the page source after interactions
        page_source = driver.page_source
        # Close the WebDriver once done to avoid multiple instances
        driver.quit()
        # Parse the page source with Beautiful Soup
        soup = BeautifulSoup(page_source, "html.parser")
        # Extract data using Beautiful Soup methods
        data_element = soup.find("p", class_="b-product_details-composition")
        if data_element:
            return data_element.text
        else:
            return "Data element not found on the page."
    except Exception as e:
        return f"An error occurred: {str(e)}"

# URL of the webpage you want to scrape
url = "https://www.nastygal.com/diamante-denim-wide-leg-jeans/BGG21619.html?_gl=1*dqvdhd*_up*MQ..*_ga*MTU0NzA2MjcuMTcxMzM5NzA0MA..*_ga_YB0PXWCT3D*MTcxMzM5NzAzOS4xLjEuMTcxMzM5NzA1NS4wLjAuMA.."
# Call the function to scrape the website
data = scrape_nasty_gal(url)
print(data)


def scrape_north_face(url, interactive_element_xpath, loaded_content_xpath, wait_time=10):
    try:
        # Set Chrome options for headless mode
        chrome_options = Options()

        # Initialize the WebDriver with headless mode
        driver = webdriver.Chrome(options=chrome_options)
        
        # Open the webpage
        driver.get(url)

        # Wait for the specified time before clicking the interactive element
        time.sleep(wait_time)  # Wait for the specified time in seconds

        # Find the interactive element
        interactive_element = driver.find_element(By.XPATH, interactive_element_xpath)
        
        # Click the interactive element
        interactive_element.click()

        # Wait for the loaded content to be visible
        loaded_element = WebDriverWait(driver, wait_time).until(
            EC.visibility_of_element_located((By.XPATH, loaded_content_xpath))
        )

        # Once loaded, scrape the content
        dynamic_content = loaded_element.text
        
        return dynamic_content
        
    except Exception as e:
        print(f"An error occurred: {str(e)}")
        return None
        
    finally:
        # Close the WebDriver
        driver.quit()

# Example usage:
url = "https://www.thenorthface.com/en-us/womens/womens-tops/womens-t-shirts-c213341/womens-short-sleeve-heavyweight-tee-pNF0A88E3?color=LK5"
interactive_element_xpath = '//*[@id="template-pdp-enabled"]/div[3]/div/div[4]/div/div/div[2]/div[13]/div[3]/div/button'
loaded_content_xpath = '//*[@id="tab-panel-name-details-content"]'

dynamic_content = scrape_north_face(url, interactive_element_xpath, loaded_content_xpath)
if dynamic_content:
    print(dynamic_content)



def scrape_north_face(url, interactive_element_xpath, loaded_content_xpath, wait_time=10):
    try:
        # Set Chrome options for headless mode
        chrome_options = Options()

        # Initialize the WebDriver with headless mode
        driver = webdriver.Chrome(options=chrome_options)
        
        # Open the webpage
        driver.get(url)

        # Wait for the specified time before clicking the interactive element
        time.sleep(wait_time)  # Wait for the specified time in seconds

        # Find the interactive element
        interactive_element = driver.find_element(By.XPATH, interactive_element_xpath)

        time.sleep(wait_time)
        
        # Click the interactive element
        interactive_element.click()

        # Wait for the loaded content to be visible
        loaded_element = WebDriverWait(driver, wait_time).until(
            EC.visibility_of_element_located((By.XPATH, loaded_content_xpath))
        )

        # Once loaded, scrape the content
        dynamic_content = loaded_element.text
        
        return dynamic_content
        
    except Exception as e:
        print(f"An error occurred: {str(e)}")
        return None
        
    finally:
        # Close the WebDriver
        driver.quit()

# Example usage:
url = "https://skims.com/products/boyfriend-loose-boxer-lily"
interactive_element_xpath = '//*[@id="essential"]/div[3]/div/div[6]/div[2]/button'
loaded_content_xpath = '//*[@id="essential"]/div[3]/div/div[6]/div[2]/div/div/div[1]/div/ul'

dynamic_content = scrape_north_face(url, interactive_element_xpath, loaded_content_xpath)
if dynamic_content:
    print(dynamic_content)




def scrape_good_american(url):
    try:
        # Set Chrome options for headless mode
        chrome_options = ChromeOptions()
        chrome_options.add_argument("--headless")
        
        # Initialize the WebDriver
        driver = Chrome(options=chrome_options)
        
        # Open the webpage
        driver.get(url)
        
        # Wait for the page to fully load
        time.sleep(5)  # Adjust the wait time as needed
        
        # Get the page source after interactions
        page_source = driver.page_source
        
        # Close the WebDriver once done to avoid multiple instances
        driver.quit()
        
        # Parse the page source with lxml and XPath
        tree = html.fromstring(page_source)
        
        # Extract data using XPath expressions
        data_element = tree.xpath('//li/strong[text()="Fabric:"]/following-sibling::text()')
        
        if data_element:
            return ''.join(data_element).strip()
        else:
            return "Data element not found on the page."
            
    except Exception as e:
        return f"An error occurred: {str(e)}"

# URL of the webpage you want to scrape
url = "https://www.goodamerican.com/products/good-legs-deep-v-clean-hem-blue609?Inseam=Regular"
# Call the function to scrape the website
data = scrape_good_american(url)
print(data)



def scrape_acne_studios(url):
    try:
        # Set Chrome options for headless mode
        chrome_options = Options()
        chrome_options.add_argument("--headless")
        # Initialize the WebDriver
        driver = webdriver.Chrome(options=chrome_options)
        driver.get(url)
        # Get the page source after interactions
        page_source = driver.page_source
        # Close the WebDriver once done to avoid multiple instances
        driver.quit()
        # Parse the page source with Beautiful Soup
        soup = BeautifulSoup(page_source, "html.parser")
        # Extract data using Beautiful Soup methods
        data_element = soup.find("h4", class_="h4 fabric-care-mfe__label")
        if data_element:
            return data_element.text
        else:
            return "Data element not found on the page."
    except Exception as e:
        return f"An error occurred: {str(e)}"

# URL of the webpage you want to scrape
url = "https://www.hollisterco.com/shop/us/p/low-rise-dark-wash-baggy-jeans-56195470?categoryId=12552&faceout=model&seq=03"
# Call the function to scrape the website
data = scrape_acne_studios(url)
print(data)



def scrape_theory(url):
    try:
        # Set Chrome options for headless mode
        chrome_options = ChromeOptions()
        chrome_options.add_argument("--headless")
        
        # Initialize the WebDriver
        driver = Chrome(options=chrome_options)
        
        # Open the webpage
        driver.get(url)
        
        # Wait for the page to fully load
        time.sleep(5)  # Adjust the wait time as needed
        
        # Get the page source after interactions
        page_source = driver.page_source
        
        # Close the WebDriver once done to avoid multiple instances
        driver.quit()
        
        # Parse the page source with lxml and XPath
        tree = html.fromstring(page_source)
        
        # Extract data using XPath expressions
        data_element = tree.xpath("//ul[@class='list-style-dashed']/li/text()")
        
        if data_element:
            return '\n'.join(data_element).strip()
        else:
            return "Data element not found on the page."
            
    except Exception as e:
        return f"An error occurred: {str(e)}"

# URL of the webpage you want to scrape
url = "https://www.theory.com/treeca-pant-in-good-wool/H0101234_EA2.html"
# Call the function to scrape the website
data = scrape_theory(url)
print(data)




def scrape_theory(url):
    try:
        # Set Chrome options for headless mode
        chrome_options = ChromeOptions()
        chrome_options.add_argument("--headless")
        
        # Initialize the WebDriver
        driver = Chrome(options=chrome_options)
        
        # Open the webpage
        driver.get(url)
        
        # Wait for the page to fully load
        time.sleep(5)  # Adjust the wait time as needed
        
        # Get the page source after interactions
        page_source = driver.page_source
        
        # Close the WebDriver once done to avoid multiple instances
        driver.quit()
        
        # Parse the page source with lxml and XPath
        tree = html.fromstring(page_source)
        
        # Extract data using XPath expressions
        data_element = tree.xpath('//*[@id="productDesktop"]/main/div/div[3]/div[1]/div[2]/div/p[1]/text()')
        
        if data_element:
            return '\n'.join(data_element).strip()
        else:
            return "Data element not found on the page."
            
    except Exception as e:
        return f"An error occurred: {str(e)}"

# URL of the webpage you want to scrape
url = "https://shop.mango.com/us/women/blazers-blazers/100-linen-suit-blazer_67077112.html?c=99"
# Call the function to scrape the website
data = scrape_theory(url)
print(data)




def scrape_patagonia(url):
    try:
        # Set Chrome options for headless mode
        chrome_options = ChromeOptions()
        chrome_options.add_argument("--headless")
        
        # Initialize the WebDriver
        driver = Chrome(options=chrome_options)
        
        # Open the webpage
        driver.get(url)
        
        # Wait for the page to fully load
        time.sleep(5)  # Adjust the wait time as needed
        
        # Get the page source after interactions
        page_source = driver.page_source
        
        # Close the WebDriver once done to avoid multiple instances
        driver.quit()
        
        # Parse the page source with lxml and XPath
        tree = html.fromstring(page_source)
        
        # Extract data using XPath expressions
        data_element = tree.xpath("//li[@class='pdp__content-feature']/p[@class='content-feature__description']/text()")
        
        if data_element:
            return '\n'.join(data_element).strip()
        else:
            return "Data element not found on the page."
            
    except Exception as e:
        return f"An error occurred: {str(e)}"

# URL of the webpage you want to scrape
url = "https://www.patagonia.com/product/mens-classic-retro-x-fleece-jacket/195699845640.html?s_kwcid=17928&utm_source=google&utm_medium=cpc&utm_campaign=BB_Ecomm_Shopping_ALL_WBSP_SaleKWs&gad_source=1&gclid=CjwKCAjw5v2wBhBrEiwAXDDoJd8XQGUW_sof8fEfoDRQUdlBPIlrxyB6fyngvB0--73eUmTbzjhBZBoCa84QAvD_BwE"
# Call the function to scrape the website
data = scrape_patagonia(url)
print(data)



def scrape_victorias_secret(url):
    try:
        # Set Chrome options for headless mode
        chrome_options = ChromeOptions()
        chrome_options.add_argument("--headless")
        
        # Initialize the WebDriver
        driver = Chrome(options=chrome_options)
        
        # Open the webpage
        driver.get(url)
        
        # Wait for the page to fully load
        time.sleep(5)  # Adjust the wait time as needed
        
        # Get the page source after interactions
        page_source = driver.page_source
        
        # Close the WebDriver once done to avoid multiple instances
        driver.quit()
        
        # Parse the page source with lxml and XPath
        tree = html.fromstring(page_source)
        
        # Extract data using XPath expressions
        data_element = tree.xpath("//span[@class='prism-danger-zone']/p/text()")
        
        if data_element:
            return '\n'.join(data_element).strip()
        else:
            return "Data element not found on the page."
            
    except Exception as e:
        return f"An error occurred: {str(e)}"

# URL of the webpage you want to scrape
url = "https://www.victoriassecret.com/us/pink/apparel-catalog/5000009721?brand=pink&collectionId=fe0131b4-96f7-46f6-88a2-3325ba6ed6f5&limit=180&orderBy=REC&priceType=regular&productId=77c4ee8f-ab11-48fa-8991-4e8acd67bb04&stackId=3db62f22-b572-48db-88b2-39a3259392ba&genericId=11243247&choice=3Z3G&product_position=1&stack_position=1&dataSource=manual-collection"
# Call the function to scrape the website
data = scrape_victorias_secret(url)
print(data)



def scrape_primark(url):
    try:
        # Set Chrome options for headless mode
        chrome_options = ChromeOptions()
        chrome_options.add_argument("--headless")
        
        # Initialize the WebDriver
        driver = Chrome(options=chrome_options)
        
        # Open the webpage
        driver.get(url)
        
        # Wait for the page to fully load
        time.sleep(5)  # Adjust the wait time as needed
        
        # Get the page source after interactions
        page_source = driver.page_source
        
        # Close the WebDriver once done to avoid multiple instances
        driver.quit()
        
        # Parse the page source with lxml and XPath
        tree = html.fromstring(page_source)
        
        # Extract data using XPath expressions
        data_element = tree.xpath("//p[@class='MuiTypography-root MuiTypography-body1']/text()")
        
        if data_element:
            return '\n'.join(data_element).strip()
        else:
            return "Data element not found on the page."
            
    except Exception as e:
        return f"An error occurred: {str(e)}"

# URL of the webpage you want to scrape
url = "https://www.primark.com/en-us/p/sleeveless-floral-playsuit-pink-991096807306"
# Call the function to scrape the website
data = scrape_primark(url)
print(data)



def scrape_yes_friends(url):
    try:
        # Set Chrome options for headless mode
        chrome_options = ChromeOptions()
        chrome_options.add_argument("--headless")
        
        # Initialize the WebDriver
        driver = Chrome(options=chrome_options)
        
        # Open the webpage
        driver.get(url)
        
        # Wait for the page to fully load
        time.sleep(5)  # Adjust the wait time as needed
        
        # Get the page source after interactions
        page_source = driver.page_source
        
        # Close the WebDriver once done to avoid multiple instances
        driver.quit()
        
        # Parse the page source with lxml and XPath
        tree = html.fromstring(page_source)
        
        # Extract data using XPath expressions
        data_element = tree.xpath("//div[@class='gf_product-desc gf_gs-text-paragraph-1']/ul/li/text()")
        
        if data_element:
            return '\n'.join(data_element).strip()
        else:
            return "Data element not found on the page."
            
    except Exception as e:
        return f"An error occurred: {str(e)}"

# URL of the webpage you want to scrape
url = "https://yesfriends.co/products/womens-fairtrade-organic-t-shirt?variant=44190218125526"
# Call the function to scrape the website
data = scrape_yes_friends(url)
print(data)



## version 1
def scrape_adidas(url, interactive_element_xpath, loaded_content_xpath, wait_time=10):
    try:
        # Set Chrome options for headless mode
        chrome_options = Options()

        # Initialize the WebDriver with headless mode
        driver = webdriver.Chrome(options=chrome_options)
        
        # Open the webpage
        driver.get(url)

        # Wait for the specified time before clicking the interactive element
        time.sleep(wait_time)  # Wait for the specified time in seconds

        # Find the interactive element
        interactive_element = driver.find_element(By.XPATH, interactive_element_xpath)
        
        # Click the interactive element
        interactive_element.click()

        # Wait for the loaded content to be visible
        loaded_element = WebDriverWait(driver, wait_time).until(
            EC.visibility_of_element_located((By.XPATH, loaded_content_xpath))
        )

        # Once loaded, scrape the content
        dynamic_content = loaded_element.text
        
        return dynamic_content
        
    except Exception as e:
        print(f"An error occurred: {str(e)}")
        return None
        
    finally:
        # Close the WebDriver
        driver.quit()

# Example usage:
url = "https://www.adidas.com/us/adidas-x-farm-rio-tiro-track-jacket/IQ4497.html"
interactive_element_xpath = '//*[@id="navigation-target-specifications"]/div/button'
loaded_content_xpath = '//*[@id="navigation-target-specifications"]/div/div/div/div'

dynamic_content = scrape_adidas(url, interactive_element_xpath, loaded_content_xpath)
if dynamic_content:
    print(dynamic_content)



## version 2
def scrape_adidas(url, interactive_element_xpath, loaded_content_xpath, wait_time=20):
    try:
        user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.71 Safari/537.36"

        # Set Chrome options for headless mode
        chrome_options = Options()
        chrome_options.add_argument(f"user-agent={user_agent}")
        chrome_options.add_argument("--headless")

        # Initialize the WebDriver with headless mode
        driver = webdriver.Chrome(options=chrome_options)
        
        # Open the webpage
        driver.get(url)

        # Wait for the interactive element to be visible
        WebDriverWait(driver, wait_time).until(
            EC.visibility_of_element_located((By.XPATH, interactive_element_xpath))
        )

        # Find the interactive element
        interactive_element = driver.find_element(By.XPATH, interactive_element_xpath)
        
        # Click the interactive element
        interactive_element.click()

        # Wait for the loaded content to be visible
        loaded_element = WebDriverWait(driver, wait_time).until(
            EC.visibility_of_element_located((By.XPATH, loaded_content_xpath))
        )

        # Once loaded, scrape the content
        dynamic_content = loaded_element.text
        
        return dynamic_content
        
    except Exception as e:
        print(f"An error occurred: {str(e)}")
        return None
        
    finally:
        # Close the WebDriver
        driver.quit()

# Example usage:
url = "https://www.adidas.com/us/manchester-united-23-24-home-jersey/IP1726.html"
interactive_element_xpath = '//*[@id="navigation-target-specifications"]/div/button'
loaded_content_xpath = '//*[@id="navigation-target-specifications"]/div/div/div/div'

dynamic_content = scrape_adidas(url, interactive_element_xpath, loaded_content_xpath)
if dynamic_content:
    print(dynamic_content)

