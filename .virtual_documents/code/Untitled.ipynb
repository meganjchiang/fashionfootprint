








from googleapiclient.discovery import build
import pandas as pd
from datetime import datetime
import re
import os
import config

# set up YouTube Data API 
api_key = config.API_KEY
youtube = build('youtube', 'v3', developerKey=api_key)


def get_youtube_data(brand_name):
    # get youtube videos from past 6 months
    published_after = datetime(2023, 9, 1).isoformat() + 'Z'
    published_before = datetime(2024, 4, 24).isoformat() + 'Z'
    # refined keywords more
    keywords = ['haul', 'clothing', 'clothes', 'shop', 'shopping', 'try on', 'try-on', 'review', 'styling']
    # added more links we do not want
    social_media_links = ['pinterest', 'youtube', 'twitter', 'instagram', 'tiktok',
                          'reddit', 'twitch', 'facebook', 'thmatc', 'spotify']

    # fetch initial search results with limit of 10 results per page
    search_results = []
    request = youtube.search().list(
        q=brand_name,
        part='snippet',
        type='video',
        publishedAfter=published_after,
        publishedBefore=published_before,
        maxResults=5
    )
    response = request.execute()
    next_page_token = response.get('nextPageToken')
    
    while next_page_token is not None:
        # send request to YouTube API
        request = youtube.search().list(
            q=brand_name,
            part='snippet',
            type='video',
            publishedAfter=published_after,
            publishedBefore=published_before,
            # doubles the number of results obtained,
            # getting 10 more results with each page fetched
            maxResults=5,
            pageToken=next_page_token
        )
        response = request.execute()
        # add items from response 
        search_results.extend(response.get('items',[]))
        # get next page token for pagination
        next_page_token = response.get('nextPageToken')

    # process search results to extract relevant vid data
    brand_videos = []
    for search_result in search_results:
        # gets and stores video id
        video_id = search_result['id']['videoId']
        video_response = youtube.videos().list(
            # receive snippet part of data - title, description, tags, etc.
            part="snippet",
            id=video_id
        ).execute()

        # access description field of snipper
        description = video_response['items'][0]['snippet']['description']
        # extract links from description
        links = re.findall(r'(https?://\S+)', description)
        # makes all titles lowercase so code can match on any version of title:
        title = search_result['snippet']['title'].lower()

        # filters based on brand name and fashion related keywords
        if brand_name.lower() in title and any(keyword in title for keyword in keywords):
            # filters out social media links
            filtered_links = [link for link in links if not any(keyword in link for keyword in social_media_links)]
            # get link to video
            video_link = f"https://www.youtube.com/watch?v={video_id}"
            brand_videos.append({
                'title': search_result['snippet']['title'],
                'links': filtered_links,
                'videoLink': video_link
            })

    # process data and format for csv
    brand_youtube_data = []
    for video in brand_videos:
        if video['links']:
            brand_youtube_data.append({
                'Title': video['title'],
                'Links': '\n'.join(video['links']),
                # add video link so we can scrape and match based on links!
                'VideoLink': video['videoLink']
            })

    return brand_youtube_data


brands = ['Uniqlo', 'Shein']

# dict to store YouTube data for all brands
# keys: brand names, values: data from YouTube
all_brand_youtube_data = {}

# iterate over each brand and fetch and process data
for brand in brands:
    youtube_data = get_youtube_data(brand)
    # creates key for brand's data and assigns YouTube data to it to add to dict
    all_brand_youtube_data[brand] = youtube_data

# print out the YouTube data for each brand
for brand, data in all_brand_youtube_data.items():
    print(f"Brand: {brand}")
    for video in data:
        print(f"Title: {video['Title']}")
        print(f"Links: {video['Links']}")
        print(f"Video Link: {video['VideoLink']}")
        print()
    print()















